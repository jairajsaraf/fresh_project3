{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWnT6ArosGFu"
      },
      "source": [
        "# Strategy 1: Advanced Residual + Selective Attention\n",
        "\n",
        "**Based on Professor's Top Submissions (Avg Score: 77.8)**\n",
        "\n",
        "## Architecture Features:\n",
        "- Residual connections with skip paths\n",
        "- Selective multi-head attention (1 attention block)\n",
        "- LayerNorm + BatchNorm combinations\n",
        "- Log-space prediction (log2)\n",
        "- Progressive dropout (0.3 → 0.2 → 0.1)\n",
        "- AdamW optimizer with weight decay\n",
        "\n",
        "## Data Strategy:\n",
        "- **CRITICAL: Split FIRST, then augment ONLY training data**\n",
        "- Heavy augmentation on training set: 3x multiplier\n",
        "- Techniques: Gaussian noise, perturbations, SMOTE-like interpolation\n",
        "- Validation set: UNTOUCHED (no augmentation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSFLWHbrsGFw",
        "outputId": "ca9d6a8c-e008-405f-c5f3-4b873f4a6aca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "STRATEGY 1: ADVANCED RESIDUAL + SELECTIVE ATTENTION\n",
            "======================================================================\n",
            "TensorFlow version: 2.19.0\n",
            "GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Embedding, Flatten, Concatenate,\n",
        "    Dropout, BatchNormalization, LayerNormalization, Add,\n",
        "    Reshape, MultiHeadAttention, Lambda\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        ")\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STRATEGY 1: ADVANCED RESIDUAL + SELECTIVE ATTENTION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M6_jnTDsGFw"
      },
      "source": [
        "## Step 1: Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhy5uXdNsGFx",
        "outputId": "9a72af95-c1e9-48af-bff3-c377de282f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 1: Loading Data\n",
            "----------------------------------------------------------------------\n",
            "Raw samples: 108000\n",
            "Sample: n=9, k=5, m=3, P=(5, 4)\n",
            "Target range: [2.00, 33517570.00]\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSTEP 1: Loading Data\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "with open('combined_final_n_k_m_P.pkl', 'rb') as f:\n",
        "    inputs_raw = pickle.load(f)\n",
        "\n",
        "with open('combined_final_mHeights.pkl', 'rb') as f:\n",
        "    outputs_raw = pickle.load(f)\n",
        "\n",
        "print(f\"Raw samples: {len(inputs_raw)}\")\n",
        "print(f\"Sample: n={inputs_raw[0][0]}, k={inputs_raw[0][1]}, m={inputs_raw[0][2]}, P={inputs_raw[0][3].shape}\")\n",
        "print(f\"Target range: [{np.min(outputs_raw):.2f}, {np.max(outputs_raw):.2f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl8-aDQgsGFx"
      },
      "source": [
        "## Step 2: Split Data FIRST (NO AUGMENTATION YET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4wsB0zysGFx",
        "outputId": "7a78f27a-25b4-4981-8e6c-cbc11bd264c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 2: Split Data FIRST (before augmentation)\n",
            "----------------------------------------------------------------------\n",
            "Training samples (before augmentation): 91800\n",
            "Validation samples (NO augmentation): 16200\n",
            "\n",
            "⚠️  CRITICAL: Validation data will NOT be augmented (prevents data leakage)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSTEP 2: Split Data FIRST (before augmentation)\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Create stratification labels\n",
        "stratify_labels = [sample[1] * 10 + sample[2] for sample in inputs_raw]\n",
        "\n",
        "# Split FIRST\n",
        "inputs_train, inputs_val, outputs_train, outputs_val = train_test_split(\n",
        "    inputs_raw, outputs_raw,\n",
        "    test_size=0.15,\n",
        "    random_state=42,\n",
        "    stratify=stratify_labels\n",
        ")\n",
        "\n",
        "print(f\"Training samples (before augmentation): {len(inputs_train)}\")\n",
        "print(f\"Validation samples (NO augmentation): {len(inputs_val)}\")\n",
        "print(\"\\n⚠️  CRITICAL: Validation data will NOT be augmented (prevents data leakage)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyzsyEw_sGFx"
      },
      "source": [
        "## Step 3: Augment ONLY Training Data (3x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmjZPwjwsGFx",
        "outputId": "04a77329-f7b8-4fb0-9671-957085293a2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 3: Augment ONLY Training Data (3x multiplier)\n",
            "----------------------------------------------------------------------\n",
            "Training samples after augmentation: 275400\n",
            "Augmentation ratio: 3.00x\n",
            "Validation samples (unchanged): 16200\n",
            "\n",
            "✅ NO DATA LEAKAGE: Validation set is completely independent\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSTEP 3: Augment ONLY Training Data (3x multiplier)\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "def augment_sample_gaussian(n, k, m, P, target, noise_level=0.03):\n",
        "    P_aug = P.copy().astype(np.float32)\n",
        "    noise = np.random.normal(0, noise_level, P_aug.shape)\n",
        "    P_aug = P_aug + noise * np.std(P_aug)\n",
        "    return [n, k, m, P_aug], target\n",
        "\n",
        "def augment_sample_perturbation(n, k, m, P, target, strength=0.02):\n",
        "    P_aug = P.copy().astype(np.float32)\n",
        "    perturbation = np.random.uniform(-strength, strength, P_aug.shape)\n",
        "    P_aug = P_aug * (1 + perturbation)\n",
        "    return [n, k, m, P_aug], target\n",
        "\n",
        "def augment_sample_interpolation(sample1, sample2, target1, target2):\n",
        "    alpha = np.random.uniform(0.3, 0.7)\n",
        "    n, k, m = sample1[0], sample1[1], sample1[2]\n",
        "    P1, P2 = sample1[3], sample2[3]\n",
        "    if sample1[1] == sample2[1] and sample1[2] == sample2[2]:\n",
        "        P_new = alpha * P1.astype(np.float32) + (1 - alpha) * P2.astype(np.float32)\n",
        "        target_new = alpha * target1 + (1 - alpha) * target2\n",
        "        return [n, k, m, P_new], target_new\n",
        "    return None, None\n",
        "\n",
        "# Group TRAINING samples by (k,m)\n",
        "train_groups = defaultdict(list)\n",
        "for i, sample in enumerate(inputs_train):\n",
        "    k, m = sample[1], sample[2]\n",
        "    train_groups[(k, m)].append(i)\n",
        "\n",
        "inputs_train_aug = []\n",
        "outputs_train_aug = []\n",
        "\n",
        "# Augment ONLY training data\n",
        "for i, (sample, target) in enumerate(zip(inputs_train, outputs_train)):\n",
        "    n, k, m, P = sample\n",
        "\n",
        "    # Keep original\n",
        "    inputs_train_aug.append(sample)\n",
        "    outputs_train_aug.append(target)\n",
        "\n",
        "    # Aug 1: Gaussian noise\n",
        "    aug1, tgt1 = augment_sample_gaussian(n, k, m, P, target)\n",
        "    inputs_train_aug.append(aug1)\n",
        "    outputs_train_aug.append(tgt1)\n",
        "\n",
        "    # Aug 2: Perturbation or interpolation\n",
        "    if np.random.rand() < 0.5:\n",
        "        aug2, tgt2 = augment_sample_perturbation(n, k, m, P, target)\n",
        "        inputs_train_aug.append(aug2)\n",
        "        outputs_train_aug.append(tgt2)\n",
        "    else:\n",
        "        group_indices = train_groups[(k, m)]\n",
        "        if len(group_indices) > 1:\n",
        "            j = np.random.choice([idx for idx in group_indices if idx != i])\n",
        "            aug2, tgt2 = augment_sample_interpolation(\n",
        "                sample, inputs_train[j], target, outputs_train[j]\n",
        "            )\n",
        "            if aug2 is not None:\n",
        "                inputs_train_aug.append(aug2)\n",
        "                outputs_train_aug.append(tgt2)\n",
        "            else:\n",
        "                aug2, tgt2 = augment_sample_perturbation(n, k, m, P, target)\n",
        "                inputs_train_aug.append(aug2)\n",
        "                outputs_train_aug.append(tgt2)\n",
        "        else:\n",
        "            aug2, tgt2 = augment_sample_perturbation(n, k, m, P, target)\n",
        "            inputs_train_aug.append(aug2)\n",
        "            outputs_train_aug.append(tgt2)\n",
        "\n",
        "print(f\"Training samples after augmentation: {len(inputs_train_aug)}\")\n",
        "print(f\"Augmentation ratio: {len(inputs_train_aug) / len(inputs_train):.2f}x\")\n",
        "print(f\"Validation samples (unchanged): {len(inputs_val)}\")\n",
        "print(\"\\n✅ NO DATA LEAKAGE: Validation set is completely independent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOsPJT17sGFy"
      },
      "source": [
        "## Step 4: Prepare Data for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWdyp6eRsGFy",
        "outputId": "8279408c-0c95-4263-fe37-14cd19c5f0e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 4: Prepare Data for Training\n",
            "----------------------------------------------------------------------\n",
            "Training: n=(275400, 1), k=(275400, 1), m=(275400, 1), P=(275400, 20), y=(275400,)\n",
            "Validation: n=(16200, 1), k=(16200, 1), m=(16200, 1), P=(16200, 20), y=(16200,)\n",
            "P train: mean=0.0000, std=1.0000\n",
            "P val: mean=0.0015, std=1.0427\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSTEP 4: Prepare Data for Training\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "def prepare_data(inputs, outputs):\n",
        "    n_vals = []\n",
        "    k_vals = []\n",
        "    m_vals = []\n",
        "    P_flat = []\n",
        "\n",
        "    for sample in inputs:\n",
        "        n_vals.append(sample[0])\n",
        "        k_vals.append(sample[1])\n",
        "        m_vals.append(sample[2])\n",
        "        P_flat.append(sample[3].flatten())\n",
        "\n",
        "    n_vals = np.array(n_vals, dtype=np.float32).reshape(-1, 1)\n",
        "    k_vals = np.array(k_vals, dtype=np.int32).reshape(-1, 1)\n",
        "    m_vals = np.array(m_vals, dtype=np.int32).reshape(-1, 1)\n",
        "    outputs_arr = np.array(outputs, dtype=np.float32)\n",
        "\n",
        "    # Pad P matrices\n",
        "    max_p_size = max(len(p) for p in P_flat)\n",
        "    P_padded = []\n",
        "    for p in P_flat:\n",
        "        if len(p) < max_p_size:\n",
        "            padded = np.zeros(max_p_size, dtype=np.float32)\n",
        "            padded[:len(p)] = p\n",
        "            P_padded.append(padded)\n",
        "        else:\n",
        "            P_padded.append(p)\n",
        "\n",
        "    P_arr = np.array(P_padded, dtype=np.float32)\n",
        "    outputs_arr = np.maximum(outputs_arr, 1.0)\n",
        "\n",
        "    return n_vals, k_vals, m_vals, P_arr, outputs_arr\n",
        "\n",
        "n_train, k_train, m_train, P_train, y_train = prepare_data(inputs_train_aug, outputs_train_aug)\n",
        "n_val, k_val, m_val, P_val, y_val = prepare_data(inputs_val, outputs_val)\n",
        "\n",
        "# Normalize P matrices (fit on training, transform both)\n",
        "scaler = StandardScaler()\n",
        "P_train = scaler.fit_transform(P_train)\n",
        "P_val = scaler.transform(P_val)  # Use training scaler\n",
        "\n",
        "print(f\"Training: n={n_train.shape}, k={k_train.shape}, m={m_train.shape}, P={P_train.shape}, y={y_train.shape}\")\n",
        "print(f\"Validation: n={n_val.shape}, k={k_val.shape}, m={m_val.shape}, P={P_val.shape}, y={y_val.shape}\")\n",
        "print(f\"P train: mean={P_train.mean():.4f}, std={P_train.std():.4f}\")\n",
        "print(f\"P val: mean={P_val.mean():.4f}, std={P_val.std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWhLhvQysGFy"
      },
      "source": [
        "## Step 5: Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RiA6nIaCsGFy",
        "outputId": "3db13927-ab4e-4a0d-8d97-af33d62da631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 5: Building Advanced Residual + Attention Model\n",
            "----------------------------------------------------------------------\n",
            "Parameters: 6,163,105\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"strategy1_advanced_residual_attention\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"strategy1_advanced_residual_attention\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ P_flat (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m5,376\u001b[0m │ P_flat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │        \u001b[38;5;34m512\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ layer_normalizat… │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m131,584\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m2,048\u001b[0m │ layer_normalizat… │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ k (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ m (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m224\u001b[0m │ k[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m192\u001b[0m │ m[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │    \u001b[38;5;34m525,568\u001b[0m │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ n (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m577\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ n[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│                     │                   │            │ flatten_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                     │                   │            │ flatten_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │    \u001b[38;5;34m591,872\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │      \u001b[38;5;34m2,048\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │      \u001b[38;5;34m4,096\u001b[0m │ layer_normalizat… │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │  \u001b[38;5;34m1,049,600\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │      \u001b[38;5;34m2,048\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │  \u001b[38;5;34m1,049,600\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │      \u001b[38;5;34m2,048\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
              "│                     │                   │            │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │  \u001b[38;5;34m1,049,600\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │      \u001b[38;5;34m2,048\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │  \u001b[38;5;34m1,049,600\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │      \u001b[38;5;34m2,048\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
              "│                     │                   │            │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m524,800\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m131,328\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │        \u001b[38;5;34m512\u001b[0m │ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m256\u001b[0m │ dense_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ P_flat (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,376</span> │ P_flat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ layer_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ layer_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ k (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ m (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> │ k[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │ m[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,568</span> │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">577</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ n[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│                     │                   │            │ flatten_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                     │                   │            │ flatten_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">591,872</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │ layer_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,600</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,600</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
              "│                     │                   │            │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,600</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,600</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
              "│                     │                   │            │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ dense_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,163,105\u001b[0m (23.51 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,163,105</span> (23.51 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,159,521\u001b[0m (23.50 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,159,521</span> (23.50 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,584\u001b[0m (14.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,584</span> (14.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(\"\\nSTEP 5: Building Advanced Residual + Attention Model\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "def build_model(p_shape, k_vocab_size=7, m_vocab_size=6):\n",
        "    n_input = Input(shape=(1,), name='n')\n",
        "    k_input = Input(shape=(1,), name='k', dtype=tf.int32)\n",
        "    m_input = Input(shape=(1,), name='m', dtype=tf.int32)\n",
        "    P_input = Input(shape=(p_shape,), name='P_flat')\n",
        "\n",
        "    k_embed = Flatten()(Embedding(k_vocab_size, 32)(k_input))\n",
        "    m_embed = Flatten()(Embedding(m_vocab_size, 32)(m_input))\n",
        "\n",
        "    x = Dense(256, activation='gelu')(P_input)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Dense(512, activation='gelu')(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x_attn = Reshape((1, 512))(x)\n",
        "    x_attn = MultiHeadAttention(num_heads=4, key_dim=64, dropout=0.1)(x_attn, x_attn)\n",
        "    x_attn = Flatten()(x_attn)\n",
        "\n",
        "    combined = Concatenate()([n_input, k_embed, m_embed, x_attn])\n",
        "\n",
        "    x = Dense(1024, activation='gelu')(combined)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    residual = x\n",
        "    x = Dense(1024, activation='gelu')(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(1024, activation='gelu')(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Add()([x, residual])\n",
        "\n",
        "    residual = x\n",
        "    x = Dense(1024, activation='gelu')(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(1024, activation='gelu')(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Add()([x, residual])\n",
        "\n",
        "    x = Dense(512, activation='gelu')(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = Dense(256, activation='gelu')(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "\n",
        "    x = Dense(128, activation='gelu')(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "\n",
        "    log2_pred = Dense(1, activation='linear')(x)\n",
        "    log2_positive = Lambda(lambda z: tf.nn.softplus(z))(log2_pred)\n",
        "    m_height = Lambda(lambda z: tf.pow(2.0, z))(log2_positive)\n",
        "\n",
        "    return Model(inputs=[n_input, k_input, m_input, P_input], outputs=m_height,\n",
        "                 name='strategy1_advanced_residual_attention')\n",
        "\n",
        "model = build_model(P_train.shape[1], k_vocab_size=k_train.max()+1, m_vocab_size=m_train.max()+1)\n",
        "print(f\"Parameters: {model.count_params():,}\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EztY8czgsGFy"
      },
      "source": [
        "## Step 6: Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI2w46F6sGFy",
        "outputId": "e7af7efe-07bf-4d94-aded-a62af3803e5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 6: Compile and Train\n",
            "----------------------------------------------------------------------\n",
            "Epoch 1/200\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - log2_mse_loss: 17.8421 - loss: 4.5582\n",
            "Epoch 1: val_loss improved from inf to 1.32258, saving model to strategy1_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 26ms/step - log2_mse_loss: 17.8418 - loss: 4.5559 - val_log2_mse_loss: 17.8110 - val_loss: 1.3226 - learning_rate: 0.0010\n",
            "Epoch 2/200\n",
            "\u001b[1m1071/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.6144 - loss: 1.4326\n",
            "Epoch 2: val_loss improved from 1.32258 to 1.30018, saving model to strategy1_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.6143 - loss: 1.4326 - val_log2_mse_loss: 17.6528 - val_loss: 1.3002 - learning_rate: 0.0010\n",
            "Epoch 3/200\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.6031 - loss: 1.3923\n",
            "Epoch 3: val_loss improved from 1.30018 to 1.28411, saving model to strategy1_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.6031 - loss: 1.3923 - val_log2_mse_loss: 17.9240 - val_loss: 1.2841 - learning_rate: 0.0010\n",
            "Epoch 4/200\n",
            "\u001b[1m1070/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.6128 - loss: 1.3808\n",
            "Epoch 4: val_loss improved from 1.28411 to 1.27874, saving model to strategy1_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - log2_mse_loss: 17.6127 - loss: 1.3808 - val_log2_mse_loss: 18.0834 - val_loss: 1.2787 - learning_rate: 0.0010\n",
            "Epoch 5/200\n",
            "\u001b[1m1074/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - log2_mse_loss: 17.6166 - loss: 1.3678\n",
            "Epoch 5: val_loss did not improve from 1.27874\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - log2_mse_loss: 17.6166 - loss: 1.3678 - val_log2_mse_loss: 18.0159 - val_loss: 1.2933 - learning_rate: 0.0010\n",
            "Epoch 6/200\n",
            "\u001b[1m1071/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.6190 - loss: 1.3592\n",
            "Epoch 6: val_loss improved from 1.27874 to 1.26303, saving model to strategy1_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - log2_mse_loss: 17.6189 - loss: 1.3592 - val_log2_mse_loss: 17.9267 - val_loss: 1.2630 - learning_rate: 0.0010\n",
            "Epoch 7/200\n",
            "\u001b[1m1075/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.6104 - loss: 1.3472\n",
            "Epoch 7: val_loss did not improve from 1.26303\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.6104 - loss: 1.3472 - val_log2_mse_loss: 18.1672 - val_loss: 1.2828 - learning_rate: 0.0010\n",
            "Epoch 8/200\n",
            "\u001b[1m1073/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.6183 - loss: 1.3479\n",
            "Epoch 8: val_loss did not improve from 1.26303\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.6182 - loss: 1.3479 - val_log2_mse_loss: 18.0283 - val_loss: 1.2634 - learning_rate: 0.0010\n",
            "Epoch 9/200\n",
            "\u001b[1m1074/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.6232 - loss: 1.3300\n",
            "Epoch 9: val_loss did not improve from 1.26303\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.6232 - loss: 1.3300 - val_log2_mse_loss: 17.9155 - val_loss: 1.2805 - learning_rate: 0.0010\n",
            "Epoch 10/200\n",
            "\u001b[1m1073/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.6303 - loss: 1.3281\n",
            "Epoch 10: val_loss did not improve from 1.26303\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.6303 - loss: 1.3281 - val_log2_mse_loss: 18.0050 - val_loss: 1.2685 - learning_rate: 0.0010\n",
            "Epoch 11/200\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.6325 - loss: 1.3158\n",
            "Epoch 11: val_loss did not improve from 1.26303\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.6325 - loss: 1.3158 - val_log2_mse_loss: 18.1718 - val_loss: 1.2683 - learning_rate: 0.0010\n",
            "Epoch 12/200\n",
            "\u001b[1m1073/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - log2_mse_loss: 17.6333 - loss: 1.3106\n",
            "Epoch 12: val_loss improved from 1.26303 to 1.26157, saving model to strategy1_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - log2_mse_loss: 17.6332 - loss: 1.3106 - val_log2_mse_loss: 17.8516 - val_loss: 1.2616 - learning_rate: 0.0010\n",
            "Epoch 13/200\n",
            "\u001b[1m1073/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.6307 - loss: 1.3010\n",
            "Epoch 13: val_loss did not improve from 1.26157\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.6306 - loss: 1.3010 - val_log2_mse_loss: 17.8795 - val_loss: 1.2637 - learning_rate: 0.0010\n",
            "Epoch 14/200\n",
            "\u001b[1m1074/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.6321 - loss: 1.2932\n",
            "Epoch 14: val_loss improved from 1.26157 to 1.25113, saving model to strategy1_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - log2_mse_loss: 17.6321 - loss: 1.2932 - val_log2_mse_loss: 17.9213 - val_loss: 1.2511 - learning_rate: 0.0010\n",
            "Epoch 15/200\n",
            "\u001b[1m1075/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.6335 - loss: 1.2883\n",
            "Epoch 15: val_loss did not improve from 1.25113\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.6335 - loss: 1.2883 - val_log2_mse_loss: 18.0223 - val_loss: 1.2582 - learning_rate: 0.0010\n",
            "Epoch 16/200\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.6381 - loss: 1.2811\n",
            "Epoch 16: val_loss improved from 1.25113 to 1.24925, saving model to strategy1_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.6381 - loss: 1.2811 - val_log2_mse_loss: 17.9115 - val_loss: 1.2492 - learning_rate: 0.0010\n",
            "Epoch 17/200\n",
            "\u001b[1m1071/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.6365 - loss: 1.2714\n",
            "Epoch 17: val_loss improved from 1.24925 to 1.24921, saving model to strategy1_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - log2_mse_loss: 17.6365 - loss: 1.2714 - val_log2_mse_loss: 18.0804 - val_loss: 1.2492 - learning_rate: 0.0010\n",
            "Epoch 18/200\n",
            "\u001b[1m1070/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.6512 - loss: 1.2666\n",
            "Epoch 18: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.6512 - loss: 1.2665 - val_log2_mse_loss: 17.7229 - val_loss: 1.2593 - learning_rate: 0.0010\n",
            "Epoch 19/200\n",
            "\u001b[1m1071/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.6503 - loss: 1.2519\n",
            "Epoch 19: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.6503 - loss: 1.2519 - val_log2_mse_loss: 17.9690 - val_loss: 1.2562 - learning_rate: 0.0010\n",
            "Epoch 20/200\n",
            "\u001b[1m1074/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.6543 - loss: 1.2414\n",
            "Epoch 20: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.6544 - loss: 1.2414 - val_log2_mse_loss: 18.1141 - val_loss: 1.2740 - learning_rate: 0.0010\n",
            "Epoch 21/200\n",
            "\u001b[1m1072/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.6754 - loss: 1.2319\n",
            "Epoch 21: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.6754 - loss: 1.2319 - val_log2_mse_loss: 17.9886 - val_loss: 1.2712 - learning_rate: 0.0010\n",
            "Epoch 22/200\n",
            "\u001b[1m1070/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.6850 - loss: 1.2198\n",
            "Epoch 22: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.6850 - loss: 1.2198 - val_log2_mse_loss: 17.8297 - val_loss: 1.2717 - learning_rate: 0.0010\n",
            "Epoch 23/200\n",
            "\u001b[1m1075/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.6882 - loss: 1.2086\n",
            "Epoch 23: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.6882 - loss: 1.2086 - val_log2_mse_loss: 17.8335 - val_loss: 1.2780 - learning_rate: 0.0010\n",
            "Epoch 24/200\n",
            "\u001b[1m1074/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.7089 - loss: 1.1848\n",
            "Epoch 24: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.7089 - loss: 1.1848 - val_log2_mse_loss: 17.9771 - val_loss: 1.3059 - learning_rate: 0.0010\n",
            "Epoch 25/200\n",
            "\u001b[1m1074/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.7127 - loss: 1.1698\n",
            "Epoch 25: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.7127 - loss: 1.1698 - val_log2_mse_loss: 17.9135 - val_loss: 1.3068 - learning_rate: 0.0010\n",
            "Epoch 26/200\n",
            "\u001b[1m1073/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.7326 - loss: 1.1516\n",
            "Epoch 26: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.7326 - loss: 1.1516 - val_log2_mse_loss: 18.0088 - val_loss: 1.3168 - learning_rate: 0.0010\n",
            "Epoch 27/200\n",
            "\u001b[1m1071/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.7475 - loss: 1.1341\n",
            "Epoch 27: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.7475 - loss: 1.1340 - val_log2_mse_loss: 18.1144 - val_loss: 1.3273 - learning_rate: 0.0010\n",
            "Epoch 28/200\n",
            "\u001b[1m1073/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.7639 - loss: 1.1223\n",
            "Epoch 28: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.7639 - loss: 1.1222 - val_log2_mse_loss: 18.1904 - val_loss: 1.3552 - learning_rate: 0.0010\n",
            "Epoch 29/200\n",
            "\u001b[1m1074/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.7823 - loss: 1.0970\n",
            "Epoch 29: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.7823 - loss: 1.0970 - val_log2_mse_loss: 18.1567 - val_loss: 1.3624 - learning_rate: 0.0010\n",
            "Epoch 30/200\n",
            "\u001b[1m1071/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.8009 - loss: 1.0763\n",
            "Epoch 30: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.8009 - loss: 1.0763 - val_log2_mse_loss: 18.4312 - val_loss: 1.3802 - learning_rate: 0.0010\n",
            "Epoch 31/200\n",
            "\u001b[1m1074/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.8164 - loss: 1.0640\n",
            "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
            "\n",
            "Epoch 31: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.8164 - loss: 1.0640 - val_log2_mse_loss: 18.3654 - val_loss: 1.4117 - learning_rate: 0.0010\n",
            "Epoch 32/200\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.8466 - loss: 1.0334\n",
            "Epoch 32: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.8466 - loss: 1.0334 - val_log2_mse_loss: 17.8811 - val_loss: 1.3973 - learning_rate: 7.0000e-04\n",
            "Epoch 33/200\n",
            "\u001b[1m1070/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.8407 - loss: 1.0260\n",
            "Epoch 33: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.8407 - loss: 1.0259 - val_log2_mse_loss: 18.1300 - val_loss: 1.4002 - learning_rate: 7.0000e-04\n",
            "Epoch 34/200\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.8636 - loss: 1.0049\n",
            "Epoch 34: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.8636 - loss: 1.0049 - val_log2_mse_loss: 18.2543 - val_loss: 1.3975 - learning_rate: 7.0000e-04\n",
            "Epoch 35/200\n",
            "\u001b[1m1075/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.8802 - loss: 0.9952\n",
            "Epoch 35: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.8802 - loss: 0.9952 - val_log2_mse_loss: 18.1488 - val_loss: 1.4042 - learning_rate: 7.0000e-04\n",
            "Epoch 36/200\n",
            "\u001b[1m1072/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.8862 - loss: 0.9810\n",
            "Epoch 36: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.8861 - loss: 0.9810 - val_log2_mse_loss: 18.1322 - val_loss: 1.4173 - learning_rate: 7.0000e-04\n",
            "Epoch 37/200\n",
            "\u001b[1m1072/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.8854 - loss: 0.9753\n",
            "Epoch 37: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.8854 - loss: 0.9752 - val_log2_mse_loss: 18.3167 - val_loss: 1.3997 - learning_rate: 7.0000e-04\n",
            "Epoch 38/200\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.8992 - loss: 0.9676\n",
            "Epoch 38: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.8992 - loss: 0.9676 - val_log2_mse_loss: 18.2626 - val_loss: 1.4030 - learning_rate: 7.0000e-04\n",
            "Epoch 39/200\n",
            "\u001b[1m1071/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.9153 - loss: 0.9538\n",
            "Epoch 39: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - log2_mse_loss: 17.9152 - loss: 0.9538 - val_log2_mse_loss: 18.1555 - val_loss: 1.4142 - learning_rate: 7.0000e-04\n",
            "Epoch 40/200\n",
            "\u001b[1m1070/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.9021 - loss: 0.9526\n",
            "Epoch 40: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.9021 - loss: 0.9526 - val_log2_mse_loss: 18.3733 - val_loss: 1.4619 - learning_rate: 7.0000e-04\n",
            "Epoch 41/200\n",
            "\u001b[1m1073/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - log2_mse_loss: 17.9185 - loss: 0.9421\n",
            "Epoch 41: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - log2_mse_loss: 17.9184 - loss: 0.9421 - val_log2_mse_loss: 18.1930 - val_loss: 1.4129 - learning_rate: 7.0000e-04\n",
            "Epoch 42/200\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.9250 - loss: 0.9333\n",
            "Epoch 42: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - log2_mse_loss: 17.9249 - loss: 0.9333 - val_log2_mse_loss: 18.1643 - val_loss: 1.4746 - learning_rate: 7.0000e-04\n",
            "Epoch 43/200\n",
            "\u001b[1m1070/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.9355 - loss: 0.9241\n",
            "Epoch 43: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.9354 - loss: 0.9241 - val_log2_mse_loss: 18.3569 - val_loss: 1.4555 - learning_rate: 7.0000e-04\n",
            "Epoch 44/200\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.9262 - loss: 0.9283\n",
            "Epoch 44: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.9262 - loss: 0.9283 - val_log2_mse_loss: 18.3639 - val_loss: 1.4860 - learning_rate: 7.0000e-04\n",
            "Epoch 45/200\n",
            "\u001b[1m1075/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.9353 - loss: 0.9136\n",
            "Epoch 45: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.9353 - loss: 0.9136 - val_log2_mse_loss: 18.5066 - val_loss: 1.4610 - learning_rate: 7.0000e-04\n",
            "Epoch 46/200\n",
            "\u001b[1m1073/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.9444 - loss: 0.9090\n",
            "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
            "\n",
            "Epoch 46: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.9444 - loss: 0.9090 - val_log2_mse_loss: 18.2517 - val_loss: 1.4352 - learning_rate: 7.0000e-04\n",
            "Epoch 47/200\n",
            "\u001b[1m1075/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - log2_mse_loss: 17.9543 - loss: 0.9048\n",
            "Epoch 47: val_loss did not improve from 1.24921\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - log2_mse_loss: 17.9543 - loss: 0.9048 - val_log2_mse_loss: 18.2660 - val_loss: 1.4488 - learning_rate: 4.9000e-04\n",
            "Epoch 47: early stopping\n",
            "Restoring model weights from the end of the best epoch: 17.\n",
            "\n",
            "Training completed!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSTEP 6: Compile and Train\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "def log2_mse_loss(y_true, y_pred):\n",
        "    epsilon = 1e-7\n",
        "    y_true = tf.maximum(y_true, epsilon)\n",
        "    y_pred = tf.maximum(y_pred, epsilon)\n",
        "    log2_true = tf.math.log(y_true) / tf.math.log(2.0)\n",
        "    log2_pred = tf.math.log(y_pred) / tf.math.log(2.0)\n",
        "    return tf.reduce_mean(tf.square(log2_true - log2_pred))\n",
        "\n",
        "optimizer = AdamW(learning_rate=1e-3, weight_decay=1e-4, clipnorm=1.0)\n",
        "model.compile(optimizer=optimizer, loss=log2_mse_loss, metrics=[log2_mse_loss])\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=15, min_lr=1e-6, verbose=1),\n",
        "    ModelCheckpoint('strategy1_best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    [n_train, k_train, m_train, P_train], y_train,\n",
        "    validation_data=([n_val, k_val, m_val, P_val], y_val),\n",
        "    epochs=200, batch_size=256, callbacks=callbacks, verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTraining completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whsd45plsGFy"
      },
      "source": [
        "## Step 7: Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmMwKh1FsGFy",
        "outputId": "332f3ac6-c5a9-4b72-ceb2-6b87e1508470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 7: Evaluation\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Validation log2-MSE: 1.249205\n",
            "Prediction range: [5.45, 29614.96]\n",
            "\n",
            "Per-Group Performance:\n",
            "Group        n_val    log2-MSE    \n",
            "----------------------------------------\n",
            "k=4, m=2      1800   0.292279\n",
            "k=4, m=3      1800   0.349115\n",
            "k=4, m=4      1800   0.839172\n",
            "k=4, m=5      1800   2.630090\n",
            "k=5, m=2      1800   0.308730\n",
            "k=5, m=3      1800   0.892862\n",
            "k=5, m=4      1800   2.622234\n",
            "k=6, m=2      1800   0.702601\n",
            "k=6, m=3      1800   2.605762\n",
            "\n",
            "======================================================================\n",
            "STRATEGY 1 COMPLETE\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSTEP 7: Evaluation\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "model.load_weights('strategy1_best_model.h5')\n",
        "y_pred_val = model.predict([n_val, k_val, m_val, P_val], verbose=0).flatten()\n",
        "\n",
        "def compute_log2_mse(y_true, y_pred):\n",
        "    epsilon = 1e-7\n",
        "    y_true = np.maximum(y_true, epsilon)\n",
        "    y_pred = np.maximum(y_pred, epsilon)\n",
        "    return np.mean((np.log2(y_true) - np.log2(y_pred)) ** 2)\n",
        "\n",
        "val_log2_mse = compute_log2_mse(y_val, y_pred_val)\n",
        "\n",
        "print(f\"\\nValidation log2-MSE: {val_log2_mse:.6f}\")\n",
        "print(f\"Prediction range: [{y_pred_val.min():.2f}, {y_pred_val.max():.2f}]\")\n",
        "\n",
        "group_metrics = defaultdict(lambda: {'true': [], 'pred': []})\n",
        "for i in range(len(y_val)):\n",
        "    k, m = k_val[i, 0], m_val[i, 0]\n",
        "    group_metrics[(k, m)]['true'].append(y_val[i])\n",
        "    group_metrics[(k, m)]['pred'].append(y_pred_val[i])\n",
        "\n",
        "print(\"\\nPer-Group Performance:\")\n",
        "print(f\"{'Group':<12} {'n_val':<8} {'log2-MSE':<12}\")\n",
        "print(\"-\"*40)\n",
        "for (k, m), data in sorted(group_metrics.items()):\n",
        "    group_mse = compute_log2_mse(np.array(data['true']), np.array(data['pred']))\n",
        "    print(f\"k={k}, m={m}    {len(data['true']):6d}   {group_mse:.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STRATEGY 1 COMPLETE\")\n",
        "print(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}