{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGd1LcvhvTiE"
      },
      "source": [
        "# Strategy 2: Dense Networks with Progressive Reduction\n",
        "\n",
        "**Based on Professor's Top Submissions (Avg Score: 73.2)**\n",
        "\n",
        "## Architecture Features:\n",
        "- Dense MLP with progressive width reduction\n",
        "- NO attention layers (simpler, faster)\n",
        "- NO residual connections (pure feedforward)\n",
        "- BatchNorm only (no LayerNorm)\n",
        "- Progressive dropout (0.3 → 0.2 → 0.1)\n",
        "- Width: 1024→768→512→384→256→128\n",
        "- AdamW optimizer with weight decay\n",
        "\n",
        "## Data Strategy:\n",
        "- **CRITICAL: Split FIRST, then augment ONLY training data**\n",
        "- Heavy augmentation on training set: 3x multiplier\n",
        "- Techniques: Gaussian noise, perturbations, SMOTE-like interpolation\n",
        "- Validation set: UNTOUCHED (no augmentation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjQeEw4PvTiH",
        "outputId": "8dfeb0b7-b3d7-4d90-afa7-7c0def9bb1e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "STRATEGY 2: DENSE NETWORKS WITH PROGRESSIVE REDUCTION\n",
            "======================================================================\n",
            "TensorFlow version: 2.19.0\n",
            "GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Embedding, Flatten, Concatenate,\n",
        "    Dropout, BatchNormalization, Lambda\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        ")\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STRATEGY 2: DENSE NETWORKS WITH PROGRESSIVE REDUCTION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMqBlts6vTiI"
      },
      "source": [
        "## Step 1: Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVJpXhBivTiI",
        "outputId": "ef59f878-00d4-420a-b36a-0b685dcc23ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 1: Loading Data\n",
            "----------------------------------------------------------------------\n",
            "Raw samples: 108000\n",
            "Sample: n=9, k=5, m=3, P=(5, 4)\n",
            "Target range: [2.00, 33517570.00]\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSTEP 1: Loading Data\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "with open('combined_final_n_k_m_P.pkl', 'rb') as f:\n",
        "    inputs_raw = pickle.load(f)\n",
        "\n",
        "with open('combined_final_mHeights.pkl', 'rb') as f:\n",
        "    outputs_raw = pickle.load(f)\n",
        "\n",
        "print(f\"Raw samples: {len(inputs_raw)}\")\n",
        "print(f\"Sample: n={inputs_raw[0][0]}, k={inputs_raw[0][1]}, m={inputs_raw[0][2]}, P={inputs_raw[0][3].shape}\")\n",
        "print(f\"Target range: [{np.min(outputs_raw):.2f}, {np.max(outputs_raw):.2f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq7OP5hSvTiI"
      },
      "source": [
        "## Step 2: Split Data FIRST (NO AUGMENTATION YET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1n2sKLAvTiJ",
        "outputId": "4cd95adb-b737-4a3c-b8b5-1021bc35b64f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 2: Split Data FIRST (before augmentation)\n",
            "----------------------------------------------------------------------\n",
            "Training samples (before augmentation): 91800\n",
            "Validation samples (NO augmentation): 16200\n",
            "\n",
            "⚠️  CRITICAL: Validation data will NOT be augmented (prevents data leakage)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSTEP 2: Split Data FIRST (before augmentation)\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Create stratification labels\n",
        "stratify_labels = [sample[1] * 10 + sample[2] for sample in inputs_raw]\n",
        "\n",
        "# Split FIRST\n",
        "inputs_train, inputs_val, outputs_train, outputs_val = train_test_split(\n",
        "    inputs_raw, outputs_raw,\n",
        "    test_size=0.15,\n",
        "    random_state=42,\n",
        "    stratify=stratify_labels\n",
        ")\n",
        "\n",
        "print(f\"Training samples (before augmentation): {len(inputs_train)}\")\n",
        "print(f\"Validation samples (NO augmentation): {len(inputs_val)}\")\n",
        "print(\"\\n⚠️  CRITICAL: Validation data will NOT be augmented (prevents data leakage)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gDTr0LjvTiJ"
      },
      "source": [
        "## Step 3: Augment ONLY Training Data (3x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zANMcUkCvTiJ",
        "outputId": "7e80e76d-051c-4cb8-e38e-c74524c47c34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 3: Augment ONLY Training Data (3x multiplier)\n",
            "----------------------------------------------------------------------\n",
            "Training samples after augmentation: 275400\n",
            "Augmentation ratio: 3.00x\n",
            "Validation samples (unchanged): 16200\n",
            "\n",
            "✅ NO DATA LEAKAGE: Validation set is completely independent\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSTEP 3: Augment ONLY Training Data (3x multiplier)\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "def augment_sample_gaussian(n, k, m, P, target, noise_level=0.03):\n",
        "    P_aug = P.copy().astype(np.float32)\n",
        "    noise = np.random.normal(0, noise_level, P_aug.shape)\n",
        "    P_aug = P_aug + noise * np.std(P_aug)\n",
        "    return [n, k, m, P_aug], target\n",
        "\n",
        "def augment_sample_perturbation(n, k, m, P, target, strength=0.02):\n",
        "    P_aug = P.copy().astype(np.float32)\n",
        "    perturbation = np.random.uniform(-strength, strength, P_aug.shape)\n",
        "    P_aug = P_aug * (1 + perturbation)\n",
        "    return [n, k, m, P_aug], target\n",
        "\n",
        "def augment_sample_interpolation(sample1, sample2, target1, target2):\n",
        "    alpha = np.random.uniform(0.3, 0.7)\n",
        "    n, k, m = sample1[0], sample1[1], sample1[2]\n",
        "    P1, P2 = sample1[3], sample2[3]\n",
        "    if sample1[1] == sample2[1] and sample1[2] == sample2[2]:\n",
        "        P_new = alpha * P1.astype(np.float32) + (1 - alpha) * P2.astype(np.float32)\n",
        "        target_new = alpha * target1 + (1 - alpha) * target2\n",
        "        return [n, k, m, P_new], target_new\n",
        "    return None, None\n",
        "\n",
        "# Group TRAINING samples by (k,m)\n",
        "train_groups = defaultdict(list)\n",
        "for i, sample in enumerate(inputs_train):\n",
        "    k, m = sample[1], sample[2]\n",
        "    train_groups[(k, m)].append(i)\n",
        "\n",
        "inputs_train_aug = []\n",
        "outputs_train_aug = []\n",
        "\n",
        "# Augment ONLY training data\n",
        "for i, (sample, target) in enumerate(zip(inputs_train, outputs_train)):\n",
        "    n, k, m, P = sample\n",
        "\n",
        "    # Keep original\n",
        "    inputs_train_aug.append(sample)\n",
        "    outputs_train_aug.append(target)\n",
        "\n",
        "    # Aug 1: Gaussian noise\n",
        "    aug1, tgt1 = augment_sample_gaussian(n, k, m, P, target)\n",
        "    inputs_train_aug.append(aug1)\n",
        "    outputs_train_aug.append(tgt1)\n",
        "\n",
        "    # Aug 2: Perturbation or interpolation\n",
        "    if np.random.rand() < 0.5:\n",
        "        aug2, tgt2 = augment_sample_perturbation(n, k, m, P, target)\n",
        "        inputs_train_aug.append(aug2)\n",
        "        outputs_train_aug.append(tgt2)\n",
        "    else:\n",
        "        group_indices = train_groups[(k, m)]\n",
        "        if len(group_indices) > 1:\n",
        "            j = np.random.choice([idx for idx in group_indices if idx != i])\n",
        "            aug2, tgt2 = augment_sample_interpolation(\n",
        "                sample, inputs_train[j], target, outputs_train[j]\n",
        "            )\n",
        "            if aug2 is not None:\n",
        "                inputs_train_aug.append(aug2)\n",
        "                outputs_train_aug.append(tgt2)\n",
        "            else:\n",
        "                aug2, tgt2 = augment_sample_perturbation(n, k, m, P, target)\n",
        "                inputs_train_aug.append(aug2)\n",
        "                outputs_train_aug.append(tgt2)\n",
        "        else:\n",
        "            aug2, tgt2 = augment_sample_perturbation(n, k, m, P, target)\n",
        "            inputs_train_aug.append(aug2)\n",
        "            outputs_train_aug.append(tgt2)\n",
        "\n",
        "print(f\"Training samples after augmentation: {len(inputs_train_aug)}\")\n",
        "print(f\"Augmentation ratio: {len(inputs_train_aug) / len(inputs_train):.2f}x\")\n",
        "print(f\"Validation samples (unchanged): {len(inputs_val)}\")\n",
        "print(\"\\n✅ NO DATA LEAKAGE: Validation set is completely independent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EBS70BrvTiK"
      },
      "source": [
        "## Step 4: Prepare Data for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1IW1u-QvTiK",
        "outputId": "5a639eb3-7abc-443f-c36c-97273888d17e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 4: Prepare Data for Training\n",
            "----------------------------------------------------------------------\n",
            "Training: n=(275400, 1), k=(275400, 1), m=(275400, 1), P=(275400, 20), y=(275400,)\n",
            "Validation: n=(16200, 1), k=(16200, 1), m=(16200, 1), P=(16200, 20), y=(16200,)\n",
            "P train: mean=0.0000, std=1.0000\n",
            "P val: mean=0.0015, std=1.0427\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSTEP 4: Prepare Data for Training\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "def prepare_data(inputs, outputs):\n",
        "    n_vals = []\n",
        "    k_vals = []\n",
        "    m_vals = []\n",
        "    P_flat = []\n",
        "\n",
        "    for sample in inputs:\n",
        "        n_vals.append(sample[0])\n",
        "        k_vals.append(sample[1])\n",
        "        m_vals.append(sample[2])\n",
        "        P_flat.append(sample[3].flatten())\n",
        "\n",
        "    n_vals = np.array(n_vals, dtype=np.float32).reshape(-1, 1)\n",
        "    k_vals = np.array(k_vals, dtype=np.int32).reshape(-1, 1)\n",
        "    m_vals = np.array(m_vals, dtype=np.int32).reshape(-1, 1)\n",
        "    outputs_arr = np.array(outputs, dtype=np.float32)\n",
        "\n",
        "    # Pad P matrices\n",
        "    max_p_size = max(len(p) for p in P_flat)\n",
        "    P_padded = []\n",
        "    for p in P_flat:\n",
        "        if len(p) < max_p_size:\n",
        "            padded = np.zeros(max_p_size, dtype=np.float32)\n",
        "            padded[:len(p)] = p\n",
        "            P_padded.append(padded)\n",
        "        else:\n",
        "            P_padded.append(p)\n",
        "\n",
        "    P_arr = np.array(P_padded, dtype=np.float32)\n",
        "    outputs_arr = np.maximum(outputs_arr, 1.0)\n",
        "\n",
        "    return n_vals, k_vals, m_vals, P_arr, outputs_arr\n",
        "\n",
        "n_train, k_train, m_train, P_train, y_train = prepare_data(inputs_train_aug, outputs_train_aug)\n",
        "n_val, k_val, m_val, P_val, y_val = prepare_data(inputs_val, outputs_val)\n",
        "\n",
        "# Normalize P matrices (fit on training, transform both)\n",
        "scaler = StandardScaler()\n",
        "P_train = scaler.fit_transform(P_train)\n",
        "P_val = scaler.transform(P_val)  # Use training scaler\n",
        "\n",
        "print(f\"Training: n={n_train.shape}, k={k_train.shape}, m={m_train.shape}, P={P_train.shape}, y={y_train.shape}\")\n",
        "print(f\"Validation: n={n_val.shape}, k={k_val.shape}, m={m_val.shape}, P={P_val.shape}, y={y_val.shape}\")\n",
        "print(f\"P train: mean={P_train.mean():.4f}, std={P_train.std():.4f}\")\n",
        "print(f\"P val: mean={P_val.mean():.4f}, std={P_val.std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSBf9N7avTiK"
      },
      "source": [
        "## Step 5: Build Model (Dense Progressive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QNGm0OXmvTiK",
        "outputId": "3ab332cb-3e37-4cc2-8f67-5126bbae3cf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 5: Building Dense Progressive Model\n",
            "----------------------------------------------------------------------\n",
            "Parameters: 2,254,113\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"strategy2_dense_progressive\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"strategy2_dense_progressive\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ P_flat (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m5,376\u001b[0m │ P_flat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ k (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ m (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m131,584\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m224\u001b[0m │ k[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m192\u001b[0m │ m[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m2,048\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ n (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m577\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ n[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│                     │                   │            │ flatten_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                     │                   │            │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │    \u001b[38;5;34m591,872\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │      \u001b[38;5;34m4,096\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │    \u001b[38;5;34m787,200\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │      \u001b[38;5;34m3,072\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m393,728\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m2,048\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │    \u001b[38;5;34m196,992\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │      \u001b[38;5;34m1,536\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m98,560\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m512\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ P_flat (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,376</span> │ P_flat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ k (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ m (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> │ k[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │ m[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">577</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ n[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│                     │                   │            │ flatten_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                     │                   │            │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">591,872</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">787,200</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">393,728</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">196,992</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,560</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,254,113\u001b[0m (8.60 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,254,113</span> (8.60 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,246,433\u001b[0m (8.57 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,246,433</span> (8.57 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m7,680\u001b[0m (30.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,680</span> (30.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(\"\\nSTEP 5: Building Dense Progressive Model\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "def build_model(p_shape, k_vocab_size=7, m_vocab_size=6):\n",
        "    \"\"\"\n",
        "    Dense MLP with progressive width reduction\n",
        "    - NO attention layers\n",
        "    - NO residual connections\n",
        "    - BatchNorm only (no LayerNorm)\n",
        "    - Progressive reduction: 1024→768→512→384→256→128\n",
        "    \"\"\"\n",
        "    n_input = Input(shape=(1,), name='n')\n",
        "    k_input = Input(shape=(1,), name='k', dtype=tf.int32)\n",
        "    m_input = Input(shape=(1,), name='m', dtype=tf.int32)\n",
        "    P_input = Input(shape=(p_shape,), name='P_flat')\n",
        "\n",
        "    # Embeddings\n",
        "    k_embed = Flatten()(Embedding(k_vocab_size, 32)(k_input))\n",
        "    m_embed = Flatten()(Embedding(m_vocab_size, 32)(m_input))\n",
        "\n",
        "    # P processing (simple, no attention)\n",
        "    x = Dense(256, activation='gelu')(P_input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Dense(512, activation='gelu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # Combine all features\n",
        "    combined = Concatenate()([n_input, k_embed, m_embed, x])\n",
        "\n",
        "    # Dense progressive reduction: 1024→768→512→384→256→128\n",
        "    x = Dense(1024, activation='gelu')(combined)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Dense(768, activation='gelu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Dense(512, activation='gelu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = Dense(384, activation='gelu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = Dense(256, activation='gelu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "\n",
        "    x = Dense(128, activation='gelu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "\n",
        "    # Log-space prediction (same as strategy 1)\n",
        "    log2_pred = Dense(1, activation='linear')(x)\n",
        "    log2_positive = Lambda(lambda z: tf.nn.softplus(z))(log2_pred)\n",
        "    m_height = Lambda(lambda z: tf.pow(2.0, z))(log2_positive)\n",
        "\n",
        "    return Model(inputs=[n_input, k_input, m_input, P_input], outputs=m_height,\n",
        "                 name='strategy2_dense_progressive')\n",
        "\n",
        "model = build_model(P_train.shape[1], k_vocab_size=k_train.max()+1, m_vocab_size=m_train.max()+1)\n",
        "print(f\"Parameters: {model.count_params():,}\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jAdKBntvTiK"
      },
      "source": [
        "## Step 6: Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqpiOUpxvTiK",
        "outputId": "00d8a025-542c-4471-a791-6a1cc93b76cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 6: Compile and Train\n",
            "----------------------------------------------------------------------\n",
            "Epoch 1/200\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - log2_mse_loss: 30.9997 - loss: 18.9986\n",
            "Epoch 1: val_loss improved from inf to 1.97214, saving model to strategy2_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - log2_mse_loss: 30.9919 - loss: 18.9879 - val_log2_mse_loss: 20.2365 - val_loss: 1.9721 - learning_rate: 0.0010\n",
            "Epoch 2/200\n",
            "\u001b[1m1070/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - log2_mse_loss: 18.0599 - loss: 1.8715\n",
            "Epoch 2: val_loss improved from 1.97214 to 1.30224, saving model to strategy2_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 18.0586 - loss: 1.8706 - val_log2_mse_loss: 18.3020 - val_loss: 1.3022 - learning_rate: 0.0010\n",
            "Epoch 3/200\n",
            "\u001b[1m1067/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - log2_mse_loss: 17.6474 - loss: 1.5208\n",
            "Epoch 3: val_loss did not improve from 1.30224\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.6473 - loss: 1.5207 - val_log2_mse_loss: 18.5570 - val_loss: 1.3092 - learning_rate: 0.0010\n",
            "Epoch 4/200\n",
            "\u001b[1m1066/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - log2_mse_loss: 17.6307 - loss: 1.4906\n",
            "Epoch 4: val_loss improved from 1.30224 to 1.26984, saving model to strategy2_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.6306 - loss: 1.4905 - val_log2_mse_loss: 17.8408 - val_loss: 1.2698 - learning_rate: 0.0010\n",
            "Epoch 5/200\n",
            "\u001b[1m1073/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - log2_mse_loss: 17.6331 - loss: 1.4606\n",
            "Epoch 5: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.6330 - loss: 1.4606 - val_log2_mse_loss: 18.0875 - val_loss: 1.2745 - learning_rate: 0.0010\n",
            "Epoch 6/200\n",
            "\u001b[1m1066/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - log2_mse_loss: 17.6361 - loss: 1.4391\n",
            "Epoch 6: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.6361 - loss: 1.4391 - val_log2_mse_loss: 18.2740 - val_loss: 1.2769 - learning_rate: 0.0010\n",
            "Epoch 7/200\n",
            "\u001b[1m1063/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - log2_mse_loss: 17.6117 - loss: 1.4343\n",
            "Epoch 7: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.6119 - loss: 1.4342 - val_log2_mse_loss: 18.1563 - val_loss: 1.2759 - learning_rate: 0.0010\n",
            "Epoch 8/200\n",
            "\u001b[1m1065/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - log2_mse_loss: 17.6519 - loss: 1.4074\n",
            "Epoch 8: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.6519 - loss: 1.4074 - val_log2_mse_loss: 18.0987 - val_loss: 1.2710 - learning_rate: 0.0010\n",
            "Epoch 9/200\n",
            "\u001b[1m1068/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - log2_mse_loss: 17.6406 - loss: 1.3874\n",
            "Epoch 9: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.6406 - loss: 1.3874 - val_log2_mse_loss: 18.5247 - val_loss: 1.2879 - learning_rate: 0.0010\n",
            "Epoch 10/200\n",
            "\u001b[1m1075/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - log2_mse_loss: 17.6484 - loss: 1.3685\n",
            "Epoch 10: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.6484 - loss: 1.3685 - val_log2_mse_loss: 18.4349 - val_loss: 1.2906 - learning_rate: 0.0010\n",
            "Epoch 11/200\n",
            "\u001b[1m1073/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - log2_mse_loss: 17.6825 - loss: 1.3467\n",
            "Epoch 11: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.6825 - loss: 1.3467 - val_log2_mse_loss: 18.0125 - val_loss: 1.2748 - learning_rate: 0.0010\n",
            "Epoch 12/200\n",
            "\u001b[1m1064/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - log2_mse_loss: 17.6939 - loss: 1.3263\n",
            "Epoch 12: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.6940 - loss: 1.3263 - val_log2_mse_loss: 18.1579 - val_loss: 1.2822 - learning_rate: 0.0010\n",
            "Epoch 13/200\n",
            "\u001b[1m1064/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - log2_mse_loss: 17.7052 - loss: 1.3044\n",
            "Epoch 13: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.7053 - loss: 1.3044 - val_log2_mse_loss: 18.1278 - val_loss: 1.3045 - learning_rate: 0.0010\n",
            "Epoch 14/200\n",
            "\u001b[1m1062/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - log2_mse_loss: 17.7203 - loss: 1.2806\n",
            "Epoch 14: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.7203 - loss: 1.2805 - val_log2_mse_loss: 18.0609 - val_loss: 1.3106 - learning_rate: 0.0010\n",
            "Epoch 15/200\n",
            "\u001b[1m1072/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - log2_mse_loss: 17.7426 - loss: 1.2619\n",
            "Epoch 15: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.7426 - loss: 1.2619 - val_log2_mse_loss: 18.4218 - val_loss: 1.3297 - learning_rate: 0.0010\n",
            "Epoch 16/200\n",
            "\u001b[1m1069/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - log2_mse_loss: 17.7546 - loss: 1.2514\n",
            "Epoch 16: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.7546 - loss: 1.2514 - val_log2_mse_loss: 18.3543 - val_loss: 1.3296 - learning_rate: 0.0010\n",
            "Epoch 17/200\n",
            "\u001b[1m1062/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - log2_mse_loss: 17.7695 - loss: 1.2263\n",
            "Epoch 17: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.7696 - loss: 1.2262 - val_log2_mse_loss: 18.1302 - val_loss: 1.3173 - learning_rate: 0.0010\n",
            "Epoch 18/200\n",
            "\u001b[1m1063/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - log2_mse_loss: 17.7779 - loss: 1.2071\n",
            "Epoch 18: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.7780 - loss: 1.2070 - val_log2_mse_loss: 18.2922 - val_loss: 1.3349 - learning_rate: 0.0010\n",
            "Epoch 19/200\n",
            "\u001b[1m1069/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - log2_mse_loss: 17.8031 - loss: 1.1908\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
            "\n",
            "Epoch 19: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.8031 - loss: 1.1908 - val_log2_mse_loss: 18.3777 - val_loss: 1.3494 - learning_rate: 0.0010\n",
            "Epoch 20/200\n",
            "\u001b[1m1075/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - log2_mse_loss: 17.8398 - loss: 1.1679\n",
            "Epoch 20: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.8398 - loss: 1.1679 - val_log2_mse_loss: 18.1338 - val_loss: 1.3635 - learning_rate: 7.0000e-04\n",
            "Epoch 21/200\n",
            "\u001b[1m1065/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - log2_mse_loss: 17.8844 - loss: 1.1301\n",
            "Epoch 21: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.8844 - loss: 1.1300 - val_log2_mse_loss: 18.1996 - val_loss: 1.3858 - learning_rate: 7.0000e-04\n",
            "Epoch 22/200\n",
            "\u001b[1m1063/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - log2_mse_loss: 17.8888 - loss: 1.1083\n",
            "Epoch 22: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.8887 - loss: 1.1083 - val_log2_mse_loss: 18.2041 - val_loss: 1.3570 - learning_rate: 7.0000e-04\n",
            "Epoch 23/200\n",
            "\u001b[1m1062/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - log2_mse_loss: 17.8831 - loss: 1.1019\n",
            "Epoch 23: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.8832 - loss: 1.1019 - val_log2_mse_loss: 17.9238 - val_loss: 1.3480 - learning_rate: 7.0000e-04\n",
            "Epoch 24/200\n",
            "\u001b[1m1071/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - log2_mse_loss: 17.8953 - loss: 1.0883\n",
            "Epoch 24: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.8953 - loss: 1.0882 - val_log2_mse_loss: 18.0647 - val_loss: 1.3673 - learning_rate: 7.0000e-04\n",
            "Epoch 25/200\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - log2_mse_loss: 17.9127 - loss: 1.0792\n",
            "Epoch 25: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.9127 - loss: 1.0792 - val_log2_mse_loss: 18.1350 - val_loss: 1.3836 - learning_rate: 7.0000e-04\n",
            "Epoch 26/200\n",
            "\u001b[1m1075/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - log2_mse_loss: 17.9365 - loss: 1.0534\n",
            "Epoch 26: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.9365 - loss: 1.0534 - val_log2_mse_loss: 18.1640 - val_loss: 1.3818 - learning_rate: 7.0000e-04\n",
            "Epoch 27/200\n",
            "\u001b[1m1072/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - log2_mse_loss: 17.9305 - loss: 1.0521\n",
            "Epoch 27: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.9305 - loss: 1.0521 - val_log2_mse_loss: 18.0392 - val_loss: 1.3880 - learning_rate: 7.0000e-04\n",
            "Epoch 28/200\n",
            "\u001b[1m1069/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - log2_mse_loss: 17.9419 - loss: 1.0397\n",
            "Epoch 28: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.9418 - loss: 1.0397 - val_log2_mse_loss: 18.0871 - val_loss: 1.3819 - learning_rate: 7.0000e-04\n",
            "Epoch 29/200\n",
            "\u001b[1m1063/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - log2_mse_loss: 17.9487 - loss: 1.0241\n",
            "Epoch 29: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.9487 - loss: 1.0241 - val_log2_mse_loss: 18.1222 - val_loss: 1.3883 - learning_rate: 7.0000e-04\n",
            "Epoch 30/200\n",
            "\u001b[1m1063/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - log2_mse_loss: 17.9587 - loss: 1.0207\n",
            "Epoch 30: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.9587 - loss: 1.0207 - val_log2_mse_loss: 18.2057 - val_loss: 1.3942 - learning_rate: 7.0000e-04\n",
            "Epoch 31/200\n",
            "\u001b[1m1068/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - log2_mse_loss: 17.9816 - loss: 1.0137\n",
            "Epoch 31: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.9815 - loss: 1.0136 - val_log2_mse_loss: 18.1964 - val_loss: 1.4005 - learning_rate: 7.0000e-04\n",
            "Epoch 32/200\n",
            "\u001b[1m1065/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - log2_mse_loss: 17.9842 - loss: 0.9994\n",
            "Epoch 32: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.9842 - loss: 0.9994 - val_log2_mse_loss: 18.1829 - val_loss: 1.4172 - learning_rate: 7.0000e-04\n",
            "Epoch 33/200\n",
            "\u001b[1m1069/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - log2_mse_loss: 17.9913 - loss: 0.9892\n",
            "Epoch 33: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.9912 - loss: 0.9892 - val_log2_mse_loss: 18.2032 - val_loss: 1.4020 - learning_rate: 7.0000e-04\n",
            "Epoch 34/200\n",
            "\u001b[1m1075/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - log2_mse_loss: 17.9997 - loss: 0.9834\n",
            "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
            "\n",
            "Epoch 34: val_loss did not improve from 1.26984\n",
            "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - log2_mse_loss: 17.9997 - loss: 0.9834 - val_log2_mse_loss: 17.9967 - val_loss: 1.3842 - learning_rate: 7.0000e-04\n",
            "Epoch 34: early stopping\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "\n",
            "Training completed!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSTEP 6: Compile and Train\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "def log2_mse_loss(y_true, y_pred):\n",
        "    epsilon = 1e-7\n",
        "    y_true = tf.maximum(y_true, epsilon)\n",
        "    y_pred = tf.maximum(y_pred, epsilon)\n",
        "    log2_true = tf.math.log(y_true) / tf.math.log(2.0)\n",
        "    log2_pred = tf.math.log(y_pred) / tf.math.log(2.0)\n",
        "    return tf.reduce_mean(tf.square(log2_true - log2_pred))\n",
        "\n",
        "optimizer = AdamW(learning_rate=1e-3, weight_decay=1e-4, clipnorm=1.0)\n",
        "model.compile(optimizer=optimizer, loss=log2_mse_loss, metrics=[log2_mse_loss])\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=15, min_lr=1e-6, verbose=1),\n",
        "    ModelCheckpoint('strategy2_best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    [n_train, k_train, m_train, P_train], y_train,\n",
        "    validation_data=([n_val, k_val, m_val, P_val], y_val),\n",
        "    epochs=200, batch_size=256, callbacks=callbacks, verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTraining completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SikPNLAXvTiK"
      },
      "source": [
        "## Step 7: Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJQPxCk3vTiK",
        "outputId": "d2866455-88a0-4d05-8d48-04bc0bd9f49c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 7: Evaluation\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Validation log2-MSE: 1.269846\n",
            "Prediction range: [2.75, 32915.62]\n",
            "\n",
            "Per-Group Performance:\n",
            "Group        n_val    log2-MSE    \n",
            "----------------------------------------\n",
            "k=4, m=2      1800   0.356181\n",
            "k=4, m=3      1800   0.379450\n",
            "k=4, m=4      1800   0.865759\n",
            "k=4, m=5      1800   2.626638\n",
            "k=5, m=2      1800   0.340273\n",
            "k=5, m=3      1800   0.934809\n",
            "k=5, m=4      1800   2.633156\n",
            "k=6, m=2      1800   0.714991\n",
            "k=6, m=3      1800   2.577356\n",
            "\n",
            "======================================================================\n",
            "STRATEGY 2 COMPLETE\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSTEP 7: Evaluation\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "model.load_weights('strategy2_best_model.h5')\n",
        "y_pred_val = model.predict([n_val, k_val, m_val, P_val], verbose=0).flatten()\n",
        "\n",
        "def compute_log2_mse(y_true, y_pred):\n",
        "    epsilon = 1e-7\n",
        "    y_true = np.maximum(y_true, epsilon)\n",
        "    y_pred = np.maximum(y_pred, epsilon)\n",
        "    return np.mean((np.log2(y_true) - np.log2(y_pred)) ** 2)\n",
        "\n",
        "val_log2_mse = compute_log2_mse(y_val, y_pred_val)\n",
        "\n",
        "print(f\"\\nValidation log2-MSE: {val_log2_mse:.6f}\")\n",
        "print(f\"Prediction range: [{y_pred_val.min():.2f}, {y_pred_val.max():.2f}]\")\n",
        "\n",
        "group_metrics = defaultdict(lambda: {'true': [], 'pred': []})\n",
        "for i in range(len(y_val)):\n",
        "    k, m = k_val[i, 0], m_val[i, 0]\n",
        "    group_metrics[(k, m)]['true'].append(y_val[i])\n",
        "    group_metrics[(k, m)]['pred'].append(y_pred_val[i])\n",
        "\n",
        "print(\"\\nPer-Group Performance:\")\n",
        "print(f\"{'Group':<12} {'n_val':<8} {'log2-MSE':<12}\")\n",
        "print(\"-\"*40)\n",
        "for (k, m), data in sorted(group_metrics.items()):\n",
        "    group_mse = compute_log2_mse(np.array(data['true']), np.array(data['pred']))\n",
        "    print(f\"k={k}, m={m}    {len(data['true']):6d}   {group_mse:.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STRATEGY 2 COMPLETE\")\n",
        "print(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}