{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Strategy 2 (REFINED): Lightweight Dense MLP\n\n**Refinements Based on Execution Analysis:**\n\n## Issues Found in Original:\n- **EXTREME overfitting**: Best at epoch 4, val log2-MSE: 1.270\n- **Worse than Strategy 1** which was already bad\n- **3x augmentation made it worse**, not better\n- **Still too complex**: 4M parameters for simple MLP\n\n## NEW Architecture (Ultra-Simplified):\n- **Pure feedforward** (no attention, no residuals)\n- **Smaller width**: 512 max (not 1024)\n- **Fewer layers**: 512→384→256→128\n- **Higher dropout**: 0.5→0.4→0.3\n- **Strong L2 regularization**\n- **NO augmentation** - original data only\n\n## Strategy:\n- Split train/val FIRST\n- NO data augmentation\n- Simpler than Strategy 1 (~1.5M parameters)\n- Goal: Better generalization through simplicity"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pickle\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import (\n    Input, Dense, Embedding, Flatten, Concatenate,\n    Dropout, BatchNormalization, Lambda\n)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import (\n    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n)\nfrom tensorflow.keras.optimizers import AdamW\nfrom tensorflow.keras import regularizers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nprint(\"=\"*70)\nprint(\"STRATEGY 2 REFINED: LIGHTWEIGHT DENSE MLP\")\nprint(\"=\"*70)\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 1: Loading Data\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "with open('data/combined_final_n_k_m_P.pkl', 'rb') as f:\n",
    "    inputs_raw = pickle.load(f)\n",
    "\n",
    "with open('data/combined_final_mHeights.pkl', 'rb') as f:\n",
    "    outputs_raw = pickle.load(f)\n",
    "\n",
    "print(f\"Raw samples: {len(inputs_raw)}\")\n",
    "print(f\"Sample: n={inputs_raw[0][0]}, k={inputs_raw[0][1]}, m={inputs_raw[0][2]}, P={inputs_raw[0][3].shape}\")\n",
    "print(f\"Target range: [{np.min(outputs_raw):.2f}, {np.max(outputs_raw):.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Split Data (NO AUGMENTATION)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nSTEP 2: Split Data (NO AUGMENTATION - prevent overfitting)\")\nprint(\"-\"*70)\n\n# Create stratification labels\nstratify_labels = [sample[1] * 10 + sample[2] for sample in inputs_raw]\n\n# Split data\ninputs_train, inputs_val, outputs_train, outputs_val = train_test_split(\n    inputs_raw, outputs_raw,\n    test_size=0.15,\n    random_state=42,\n    stratify=stratify_labels\n)\n\nprint(f\"Training samples: {len(inputs_train)}\")\nprint(f\"Validation samples: {len(inputs_val)}\")\nprint(\"\\n✅ NO DATA AUGMENTATION - Simpler is better for this problem\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Prepare Data for Training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nSTEP 3: Prepare Data for Training\")\nprint(\"-\"*70)\n\ndef prepare_data(inputs, outputs):\n    n_vals = []\n    k_vals = []\n    m_vals = []\n    P_flat = []\n    \n    for sample in inputs:\n        n_vals.append(sample[0])\n        k_vals.append(sample[1])\n        m_vals.append(sample[2])\n        P_flat.append(sample[3].flatten())\n    \n    n_vals = np.array(n_vals, dtype=np.float32).reshape(-1, 1)\n    k_vals = np.array(k_vals, dtype=np.int32).reshape(-1, 1)\n    m_vals = np.array(m_vals, dtype=np.int32).reshape(-1, 1)\n    outputs_arr = np.array(outputs, dtype=np.float32)\n    \n    # Pad P matrices\n    max_p_size = max(len(p) for p in P_flat)\n    P_padded = []\n    for p in P_flat:\n        if len(p) < max_p_size:\n            padded = np.zeros(max_p_size, dtype=np.float32)\n            padded[:len(p)] = p\n            P_padded.append(padded)\n        else:\n            P_padded.append(p)\n    \n    P_arr = np.array(P_padded, dtype=np.float32)\n    outputs_arr = np.maximum(outputs_arr, 1.0)\n    \n    return n_vals, k_vals, m_vals, P_arr, outputs_arr\n\nn_train, k_train, m_train, P_train, y_train = prepare_data(inputs_train, outputs_train)\nn_val, k_val, m_val, P_val, y_val = prepare_data(inputs_val, outputs_val)\n\n# Normalize P matrices (fit on training, transform both)\nscaler = StandardScaler()\nP_train = scaler.fit_transform(P_train)\nP_val = scaler.transform(P_val)\n\nprint(f\"Training: n={n_train.shape}, k={k_train.shape}, m={m_train.shape}, P={P_train.shape}, y={y_train.shape}\")\nprint(f\"Validation: n={n_val.shape}, k={k_val.shape}, m={m_val.shape}, P={P_val.shape}, y={y_val.shape}\")\nprint(f\"P train: mean={P_train.mean():.4f}, std={P_train.std():.4f}\")\nprint(f\"P val: mean={P_val.mean():.4f}, std={P_val.std():.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Skipped (No Augmentation)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nSTEP 4: Skipped (removed augmentation step)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Build Ultra-Simple Dense MLP"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nSTEP 5: Building Lightweight Dense MLP\")\nprint(\"-\"*70)\n\ndef build_model(p_shape, k_vocab_size=7, m_vocab_size=6):\n    \"\"\"\n    ULTRA-SIMPLIFIED Dense MLP:\n    - NO attention\n    - NO residual connections\n    - Smaller width: 512 max (not 1024)\n    - Fewer layers: 512→384→256→128\n    - Very high dropout + L2 regularization\n    \"\"\"\n    l2_reg = regularizers.l2(2e-4)  # Stronger L2\n    \n    n_input = Input(shape=(1,), name='n')\n    k_input = Input(shape=(1,), name='k', dtype=tf.int32)\n    m_input = Input(shape=(1,), name='m', dtype=tf.int32)\n    P_input = Input(shape=(p_shape,), name='P_flat')\n\n    # Embeddings\n    k_embed = Flatten()(Embedding(k_vocab_size, 24, embeddings_regularizer=l2_reg)(k_input))\n    m_embed = Flatten()(Embedding(m_vocab_size, 24, embeddings_regularizer=l2_reg)(m_input))\n\n    # Simple P processing (no attention)\n    x = Dense(192, activation='gelu', kernel_regularizer=l2_reg)(P_input)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)  # Very high dropout\n    \n    x = Dense(256, activation='gelu', kernel_regularizer=l2_reg)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n\n    # Combine all features\n    combined = Concatenate()([n_input, k_embed, m_embed, x])\n\n    # Simple dense progression: 512→384→256→128\n    x = Dense(512, activation='gelu', kernel_regularizer=l2_reg)(combined)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    \n    x = Dense(384, activation='gelu', kernel_regularizer=l2_reg)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n    \n    x = Dense(256, activation='gelu', kernel_regularizer=l2_reg)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    \n    x = Dense(128, activation='gelu', kernel_regularizer=l2_reg)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n\n    # Log-space prediction\n    log2_pred = Dense(1, activation='linear', kernel_regularizer=l2_reg)(x)\n    log2_positive = Lambda(lambda z: tf.nn.softplus(z))(log2_pred)\n    m_height = Lambda(lambda z: tf.pow(2.0, z))(log2_positive)\n\n    return Model(inputs=[n_input, k_input, m_input, P_input], outputs=m_height,\n                 name='strategy2_lightweight')\n\nmodel = build_model(P_train.shape[1], k_vocab_size=k_train.max()+1, m_vocab_size=m_train.max()+1)\nprint(f\"Parameters: {model.count_params():,} (vs 4M in original)\")\nmodel.summary()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: Train with Strong Regularization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nSTEP 6: Compile and Train\")\nprint(\"-\"*70)\n\ndef log2_mse_loss(y_true, y_pred):\n    epsilon = 1e-7\n    y_true = tf.maximum(y_true, epsilon)\n    y_pred = tf.maximum(y_pred, epsilon)\n    log2_true = tf.math.log(y_true) / tf.math.log(2.0)\n    log2_pred = tf.math.log(y_pred) / tf.math.log(2.0)\n    return tf.reduce_mean(tf.square(log2_true - log2_pred))\n\n# Lower learning rate, stronger weight decay\noptimizer = AdamW(learning_rate=3e-4, weight_decay=3e-4, clipnorm=1.0)\nmodel.compile(optimizer=optimizer, loss=log2_mse_loss, metrics=[log2_mse_loss])\n\n# Aggressive early stopping\ncallbacks = [\n    EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True, verbose=1),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=12, min_lr=1e-6, verbose=1),\n    ModelCheckpoint('strategy2_refined_best.h5', monitor='val_loss', save_best_only=True, verbose=1)\n]\n\nprint(\"Training with NO augmentation, maximum regularization, minimal architecture...\")\nhistory = model.fit(\n    [n_train, k_train, m_train, P_train], y_train,\n    validation_data=([n_val, k_val, m_val, P_val], y_val),\n    epochs=150, batch_size=256, callbacks=callbacks, verbose=1\n)\n\nprint(\"\\nTraining completed!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nSTEP 7: Evaluation\")\nprint(\"-\"*70)\n\nmodel.load_weights('strategy2_refined_best.h5')\ny_pred_val = model.predict([n_val, k_val, m_val, P_val], verbose=0).flatten()\n\ndef compute_log2_mse(y_true, y_pred):\n    epsilon = 1e-7\n    y_true = np.maximum(y_true, epsilon)\n    y_pred = np.maximum(y_pred, epsilon)\n    return np.mean((np.log2(y_true) - np.log2(y_pred)) ** 2)\n\nval_log2_mse = compute_log2_mse(y_val, y_pred_val)\n\nprint(f\"\\nValidation log2-MSE: {val_log2_mse:.6f}\")\nprint(f\"Prediction range: [{y_pred_val.min():.2f}, {y_pred_val.max():.2f}]\")\nprint(f\"\\nCompare to original: 1.270 (peaked at epoch 4!)\")\n\ngroup_metrics = defaultdict(lambda: {'true': [], 'pred': []})\nfor i in range(len(y_val)):\n    k, m = k_val[i, 0], m_val[i, 0]\n    group_metrics[(k, m)]['true'].append(y_val[i])\n    group_metrics[(k, m)]['pred'].append(y_pred_val[i])\n\nprint(\"\\nPer-Group Performance:\")\nprint(f\"{'Group':<12} {'n_val':<8} {'log2-MSE':<12} {'vs Original':<15}\")\nprint(\"-\"*55)\n\n# Original per-group results for comparison\norig_results = {\n    (4,2): 0.356, (4,3): 0.379, (4,4): 0.866, (4,5): 2.627,\n    (5,2): 0.340, (5,3): 0.935, (5,4): 2.633,\n    (6,2): 0.715, (6,3): 2.577\n}\n\nfor (k, m), data in sorted(group_metrics.items()):\n    group_mse = compute_log2_mse(np.array(data['true']), np.array(data['pred']))\n    orig_mse = orig_results.get((k, m), 0)\n    diff = group_mse - orig_mse\n    sign = \"+\" if diff > 0 else \"\"\n    print(f\"k={k}, m={m}    {len(data['true']):6d}   {group_mse:.6f}      {sign}{diff:.3f}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"STRATEGY 2 REFINED COMPLETE\")\nprint(\"=\"*70)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}