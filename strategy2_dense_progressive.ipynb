{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Strategy 2: Dense Networks with Progressive Reduction\n\n**Based on Professor's Top Submissions (Avg Score: 73.2)**\n\n## Architecture Features:\n- Dense MLP with progressive width reduction\n- NO attention layers (simpler, faster)\n- NO residual connections (pure feedforward)\n- BatchNorm only (no LayerNorm)\n- Progressive dropout (0.3 → 0.2 → 0.1)\n- Width: 1024→768→512→384→256→128\n- AdamW optimizer with weight decay\n\n## Data Strategy:\n- **CRITICAL: Split FIRST, then augment ONLY training data**\n- Heavy augmentation on training set: 3x multiplier\n- Techniques: Gaussian noise, perturbations, SMOTE-like interpolation\n- Validation set: UNTOUCHED (no augmentation)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pickle\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import (\n    Input, Dense, Embedding, Flatten, Concatenate,\n    Dropout, BatchNormalization, Lambda\n)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import (\n    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n)\nfrom tensorflow.keras.optimizers import AdamW\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nprint(\"=\"*70)\nprint(\"STRATEGY 2: DENSE NETWORKS WITH PROGRESSIVE REDUCTION\")\nprint(\"=\"*70)\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 1: Loading Data\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "with open('data/combined_final_n_k_m_P.pkl', 'rb') as f:\n",
    "    inputs_raw = pickle.load(f)\n",
    "\n",
    "with open('data/combined_final_mHeights.pkl', 'rb') as f:\n",
    "    outputs_raw = pickle.load(f)\n",
    "\n",
    "print(f\"Raw samples: {len(inputs_raw)}\")\n",
    "print(f\"Sample: n={inputs_raw[0][0]}, k={inputs_raw[0][1]}, m={inputs_raw[0][2]}, P={inputs_raw[0][3].shape}\")\n",
    "print(f\"Target range: [{np.min(outputs_raw):.2f}, {np.max(outputs_raw):.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Split Data FIRST (NO AUGMENTATION YET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 2: Split Data FIRST (before augmentation)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Create stratification labels\n",
    "stratify_labels = [sample[1] * 10 + sample[2] for sample in inputs_raw]\n",
    "\n",
    "# Split FIRST\n",
    "inputs_train, inputs_val, outputs_train, outputs_val = train_test_split(\n",
    "    inputs_raw, outputs_raw,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    stratify=stratify_labels\n",
    ")\n",
    "\n",
    "print(f\"Training samples (before augmentation): {len(inputs_train)}\")\n",
    "print(f\"Validation samples (NO augmentation): {len(inputs_val)}\")\n",
    "print(\"\\n⚠️  CRITICAL: Validation data will NOT be augmented (prevents data leakage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Augment ONLY Training Data (3x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 3: Augment ONLY Training Data (3x multiplier)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "def augment_sample_gaussian(n, k, m, P, target, noise_level=0.03):\n",
    "    P_aug = P.copy().astype(np.float32)\n",
    "    noise = np.random.normal(0, noise_level, P_aug.shape)\n",
    "    P_aug = P_aug + noise * np.std(P_aug)\n",
    "    return [n, k, m, P_aug], target\n",
    "\n",
    "def augment_sample_perturbation(n, k, m, P, target, strength=0.02):\n",
    "    P_aug = P.copy().astype(np.float32)\n",
    "    perturbation = np.random.uniform(-strength, strength, P_aug.shape)\n",
    "    P_aug = P_aug * (1 + perturbation)\n",
    "    return [n, k, m, P_aug], target\n",
    "\n",
    "def augment_sample_interpolation(sample1, sample2, target1, target2):\n",
    "    alpha = np.random.uniform(0.3, 0.7)\n",
    "    n, k, m = sample1[0], sample1[1], sample1[2]\n",
    "    P1, P2 = sample1[3], sample2[3]\n",
    "    if sample1[1] == sample2[1] and sample1[2] == sample2[2]:\n",
    "        P_new = alpha * P1.astype(np.float32) + (1 - alpha) * P2.astype(np.float32)\n",
    "        target_new = alpha * target1 + (1 - alpha) * target2\n",
    "        return [n, k, m, P_new], target_new\n",
    "    return None, None\n",
    "\n",
    "# Group TRAINING samples by (k,m)\n",
    "train_groups = defaultdict(list)\n",
    "for i, sample in enumerate(inputs_train):\n",
    "    k, m = sample[1], sample[2]\n",
    "    train_groups[(k, m)].append(i)\n",
    "\n",
    "inputs_train_aug = []\n",
    "outputs_train_aug = []\n",
    "\n",
    "# Augment ONLY training data\n",
    "for i, (sample, target) in enumerate(zip(inputs_train, outputs_train)):\n",
    "    n, k, m, P = sample\n",
    "    \n",
    "    # Keep original\n",
    "    inputs_train_aug.append(sample)\n",
    "    outputs_train_aug.append(target)\n",
    "    \n",
    "    # Aug 1: Gaussian noise\n",
    "    aug1, tgt1 = augment_sample_gaussian(n, k, m, P, target)\n",
    "    inputs_train_aug.append(aug1)\n",
    "    outputs_train_aug.append(tgt1)\n",
    "    \n",
    "    # Aug 2: Perturbation or interpolation\n",
    "    if np.random.rand() < 0.5:\n",
    "        aug2, tgt2 = augment_sample_perturbation(n, k, m, P, target)\n",
    "        inputs_train_aug.append(aug2)\n",
    "        outputs_train_aug.append(tgt2)\n",
    "    else:\n",
    "        group_indices = train_groups[(k, m)]\n",
    "        if len(group_indices) > 1:\n",
    "            j = np.random.choice([idx for idx in group_indices if idx != i])\n",
    "            aug2, tgt2 = augment_sample_interpolation(\n",
    "                sample, inputs_train[j], target, outputs_train[j]\n",
    "            )\n",
    "            if aug2 is not None:\n",
    "                inputs_train_aug.append(aug2)\n",
    "                outputs_train_aug.append(tgt2)\n",
    "            else:\n",
    "                aug2, tgt2 = augment_sample_perturbation(n, k, m, P, target)\n",
    "                inputs_train_aug.append(aug2)\n",
    "                outputs_train_aug.append(tgt2)\n",
    "        else:\n",
    "            aug2, tgt2 = augment_sample_perturbation(n, k, m, P, target)\n",
    "            inputs_train_aug.append(aug2)\n",
    "            outputs_train_aug.append(tgt2)\n",
    "\n",
    "print(f\"Training samples after augmentation: {len(inputs_train_aug)}\")\n",
    "print(f\"Augmentation ratio: {len(inputs_train_aug) / len(inputs_train):.2f}x\")\n",
    "print(f\"Validation samples (unchanged): {len(inputs_val)}\")\n",
    "print(\"\\n✅ NO DATA LEAKAGE: Validation set is completely independent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 4: Prepare Data for Training\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "def prepare_data(inputs, outputs):\n",
    "    n_vals = []\n",
    "    k_vals = []\n",
    "    m_vals = []\n",
    "    P_flat = []\n",
    "    \n",
    "    for sample in inputs:\n",
    "        n_vals.append(sample[0])\n",
    "        k_vals.append(sample[1])\n",
    "        m_vals.append(sample[2])\n",
    "        P_flat.append(sample[3].flatten())\n",
    "    \n",
    "    n_vals = np.array(n_vals, dtype=np.float32).reshape(-1, 1)\n",
    "    k_vals = np.array(k_vals, dtype=np.int32).reshape(-1, 1)\n",
    "    m_vals = np.array(m_vals, dtype=np.int32).reshape(-1, 1)\n",
    "    outputs_arr = np.array(outputs, dtype=np.float32)\n",
    "    \n",
    "    # Pad P matrices\n",
    "    max_p_size = max(len(p) for p in P_flat)\n",
    "    P_padded = []\n",
    "    for p in P_flat:\n",
    "        if len(p) < max_p_size:\n",
    "            padded = np.zeros(max_p_size, dtype=np.float32)\n",
    "            padded[:len(p)] = p\n",
    "            P_padded.append(padded)\n",
    "        else:\n",
    "            P_padded.append(p)\n",
    "    \n",
    "    P_arr = np.array(P_padded, dtype=np.float32)\n",
    "    outputs_arr = np.maximum(outputs_arr, 1.0)\n",
    "    \n",
    "    return n_vals, k_vals, m_vals, P_arr, outputs_arr\n",
    "\n",
    "n_train, k_train, m_train, P_train, y_train = prepare_data(inputs_train_aug, outputs_train_aug)\n",
    "n_val, k_val, m_val, P_val, y_val = prepare_data(inputs_val, outputs_val)\n",
    "\n",
    "# Normalize P matrices (fit on training, transform both)\n",
    "scaler = StandardScaler()\n",
    "P_train = scaler.fit_transform(P_train)\n",
    "P_val = scaler.transform(P_val)  # Use training scaler\n",
    "\n",
    "print(f\"Training: n={n_train.shape}, k={k_train.shape}, m={m_train.shape}, P={P_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Validation: n={n_val.shape}, k={k_val.shape}, m={m_val.shape}, P={P_val.shape}, y={y_val.shape}\")\n",
    "print(f\"P train: mean={P_train.mean():.4f}, std={P_train.std():.4f}\")\n",
    "print(f\"P val: mean={P_val.mean():.4f}, std={P_val.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Build Model (Dense Progressive)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nSTEP 5: Building Dense Progressive Model\")\nprint(\"-\"*70)\n\ndef build_model(p_shape, k_vocab_size=7, m_vocab_size=6):\n    \"\"\"\n    Dense MLP with progressive width reduction\n    - NO attention layers\n    - NO residual connections\n    - BatchNorm only (no LayerNorm)\n    - Progressive reduction: 1024→768→512→384→256→128\n    \"\"\"\n    n_input = Input(shape=(1,), name='n')\n    k_input = Input(shape=(1,), name='k', dtype=tf.int32)\n    m_input = Input(shape=(1,), name='m', dtype=tf.int32)\n    P_input = Input(shape=(p_shape,), name='P_flat')\n\n    # Embeddings\n    k_embed = Flatten()(Embedding(k_vocab_size, 32)(k_input))\n    m_embed = Flatten()(Embedding(m_vocab_size, 32)(m_input))\n\n    # P processing (simple, no attention)\n    x = Dense(256, activation='gelu')(P_input)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    \n    x = Dense(512, activation='gelu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n\n    # Combine all features\n    combined = Concatenate()([n_input, k_embed, m_embed, x])\n\n    # Dense progressive reduction: 1024→768→512→384→256→128\n    x = Dense(1024, activation='gelu')(combined)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    \n    x = Dense(768, activation='gelu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    \n    x = Dense(512, activation='gelu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n    \n    x = Dense(384, activation='gelu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n    \n    x = Dense(256, activation='gelu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.1)(x)\n    \n    x = Dense(128, activation='gelu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.1)(x)\n\n    # Log-space prediction (same as strategy 1)\n    log2_pred = Dense(1, activation='linear')(x)\n    log2_positive = Lambda(lambda z: tf.nn.softplus(z))(log2_pred)\n    m_height = Lambda(lambda z: tf.pow(2.0, z))(log2_positive)\n\n    return Model(inputs=[n_input, k_input, m_input, P_input], outputs=m_height,\n                 name='strategy2_dense_progressive')\n\nmodel = build_model(P_train.shape[1], k_vocab_size=k_train.max()+1, m_vocab_size=m_train.max()+1)\nprint(f\"Parameters: {model.count_params():,}\")\nmodel.summary()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nSTEP 6: Compile and Train\")\nprint(\"-\"*70)\n\ndef log2_mse_loss(y_true, y_pred):\n    epsilon = 1e-7\n    y_true = tf.maximum(y_true, epsilon)\n    y_pred = tf.maximum(y_pred, epsilon)\n    log2_true = tf.math.log(y_true) / tf.math.log(2.0)\n    log2_pred = tf.math.log(y_pred) / tf.math.log(2.0)\n    return tf.reduce_mean(tf.square(log2_true - log2_pred))\n\noptimizer = AdamW(learning_rate=1e-3, weight_decay=1e-4, clipnorm=1.0)\nmodel.compile(optimizer=optimizer, loss=log2_mse_loss, metrics=[log2_mse_loss])\n\ncallbacks = [\n    EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True, verbose=1),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=15, min_lr=1e-6, verbose=1),\n    ModelCheckpoint('strategy2_best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n]\n\nhistory = model.fit(\n    [n_train, k_train, m_train, P_train], y_train,\n    validation_data=([n_val, k_val, m_val, P_val], y_val),\n    epochs=200, batch_size=256, callbacks=callbacks, verbose=1\n)\n\nprint(\"\\nTraining completed!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nSTEP 7: Evaluation\")\nprint(\"-\"*70)\n\nmodel.load_weights('strategy2_best_model.h5')\ny_pred_val = model.predict([n_val, k_val, m_val, P_val], verbose=0).flatten()\n\ndef compute_log2_mse(y_true, y_pred):\n    epsilon = 1e-7\n    y_true = np.maximum(y_true, epsilon)\n    y_pred = np.maximum(y_pred, epsilon)\n    return np.mean((np.log2(y_true) - np.log2(y_pred)) ** 2)\n\nval_log2_mse = compute_log2_mse(y_val, y_pred_val)\n\nprint(f\"\\nValidation log2-MSE: {val_log2_mse:.6f}\")\nprint(f\"Prediction range: [{y_pred_val.min():.2f}, {y_pred_val.max():.2f}]\")\n\ngroup_metrics = defaultdict(lambda: {'true': [], 'pred': []})\nfor i in range(len(y_val)):\n    k, m = k_val[i, 0], m_val[i, 0]\n    group_metrics[(k, m)]['true'].append(y_val[i])\n    group_metrics[(k, m)]['pred'].append(y_pred_val[i])\n\nprint(\"\\nPer-Group Performance:\")\nprint(f\"{'Group':<12} {'n_val':<8} {'log2-MSE':<12}\")\nprint(\"-\"*40)\nfor (k, m), data in sorted(group_metrics.items()):\n    group_mse = compute_log2_mse(np.array(data['true']), np.array(data['pred']))\n    print(f\"k={k}, m={m}    {len(data['true']):6d}   {group_mse:.6f}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"STRATEGY 2 COMPLETE\")\nprint(\"=\"*70)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}