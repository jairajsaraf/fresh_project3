{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Height Prediction Model Training - Run 5\n",
    "\n",
    "**Objective:** Fix Run 4 overfitting issues with simplified residual architecture\n",
    "\n",
    "## Run 5 Strategy (Based on Professor's Top Submissions)\n",
    "\n",
    "1. **REMOVE** multi-head attention (too complex, not worth it)\n",
    "2. **REMOVE** aggressive group weighting (caused 60-70% degradation in easy cases)\n",
    "3. **ADD** residual connections with LayerNorm (proven in top submissions)\n",
    "4. **ADD** progressive width reduction (1024→512→256→128)\n",
    "5. **ADD** progressive dropout pattern (0.3→0.2→0.1)\n",
    "6. **USE** mild class weighting (1.0-2.0x max, not 5.0x)\n",
    "7. **IMPROVE** learning rate schedule with exponential decay\n",
    "8. **EXTEND** training with better early stopping (patience=40, max_epochs=250)\n",
    "9. **ADD** data augmentation targeting hard/medium cases\n",
    "\n",
    "## Expected Performance\n",
    "\n",
    "- Easy cases: < 0.15 log2-MSE (match Run 2, don't degrade!)\n",
    "- Medium cases: 0.20-0.30 log2-MSE\n",
    "- Hard cases: 0.80-1.30 log2-MSE (match Run 4)\n",
    "- Overall: 0.30-0.35 (beat 0.374 target!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Embedding, Flatten, Concatenate,\n",
    "    Dropout, BatchNormalization, Add, LayerNormalization, Lambda\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint,\n",
    "    Callback, LearningRateScheduler\n",
    ")\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"HEIGHT PREDICTION MODEL - RUN 5 (SIMPLIFIED RESIDUAL)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 1: Loading Data\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "with open('data/combined_final_n_k_m_P.pkl', 'rb') as f:\n",
    "    inputs_raw = pickle.load(f)\n",
    "\n",
    "with open('data/combined_final_mHeights.pkl', 'rb') as f:\n",
    "    outputs_raw = pickle.load(f)\n",
    "\n",
    "print(f\"Raw input samples: {len(inputs_raw)}\")\n",
    "print(f\"Raw output samples: {len(outputs_raw)}\")\n",
    "if len(inputs_raw) > 0:\n",
    "    sample = inputs_raw[0]\n",
    "    print(f\"Sample structure: [n={sample[0]}, k={sample[1]}, m={sample[2]}, P_matrix shape={sample[3].shape}]\")\n",
    "print(f\"Output range: [{np.min(outputs_raw):.2f}, {np.max(outputs_raw):.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Augmentation\n",
    "\n",
    "Apply targeted augmentation:\n",
    "- Hard cases (k=5,m=4 | k=6,m=3 | k=4,m=5): +2 augmented copies each\n",
    "- Medium cases (k=4,m=4 | k=5,m=3 | k=6,m=2): +1 augmented copy each\n",
    "- Easy cases: No augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_sample(n, k, m, P_matrix, m_height, noise_scale=0.02):\n",
    "    \"\"\"\n",
    "    Augment a single sample by adding small noise\n",
    "    \"\"\"\n",
    "    # Add small noise to n (±2%)\n",
    "    n_aug = n * (1 + np.random.uniform(-noise_scale, noise_scale))\n",
    "    n_aug = max(1.0, n_aug)  # Ensure n >= 1\n",
    "\n",
    "    # Add small noise to P matrix and renormalize\n",
    "    P_aug = P_matrix.copy()\n",
    "    if P_aug.size > 0:\n",
    "        noise = np.random.normal(0, noise_scale, P_aug.shape)\n",
    "        P_aug = P_aug + noise\n",
    "        P_aug = np.clip(P_aug, 0, 1)  # Keep in [0, 1]\n",
    "\n",
    "        # Renormalize rows to sum to 1\n",
    "        if len(P_aug.shape) == 2:\n",
    "            row_sums = P_aug.sum(axis=1, keepdims=True)\n",
    "            row_sums = np.maximum(row_sums, 1e-7)\n",
    "            P_aug = P_aug / row_sums\n",
    "\n",
    "    return (n_aug, k, m, P_aug), m_height\n",
    "\n",
    "# Define augmentation multipliers\n",
    "HARD_GROUPS = [(5, 4), (6, 3), (4, 5)]\n",
    "MEDIUM_GROUPS = [(4, 4), (5, 3), (6, 2)]\n",
    "\n",
    "augmentation_config = {\n",
    "    (5, 4): 2, (6, 3): 2, (4, 5): 2,  # Hard cases: +2 copies\n",
    "    (4, 4): 1, (5, 3): 1, (6, 2): 1,  # Medium cases: +1 copy\n",
    "}\n",
    "\n",
    "inputs_augmented = []\n",
    "outputs_augmented = []\n",
    "\n",
    "print(\"Augmenting dataset...\")\n",
    "for sample, target in zip(inputs_raw, outputs_raw):\n",
    "    n, k, m, P_matrix = sample\n",
    "    \n",
    "    # Always keep original\n",
    "    inputs_augmented.append(sample)\n",
    "    outputs_augmented.append(target)\n",
    "    \n",
    "    # Add augmented copies\n",
    "    num_augmented = augmentation_config.get((k, m), 0)\n",
    "    for _ in range(num_augmented):\n",
    "        aug_sample, aug_target = augment_sample(n, k, m, P_matrix, target)\n",
    "        inputs_augmented.append(aug_sample)\n",
    "        outputs_augmented.append(aug_target)\n",
    "\n",
    "print(f\"Original samples: {len(inputs_raw)}\")\n",
    "print(f\"Augmented samples: {len(inputs_augmented)}\")\n",
    "print(f\"Augmentation ratio: {len(inputs_augmented) / len(inputs_raw):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 3: Preparing Data for Training\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "inputs_rebalanced = inputs_augmented\n",
    "outputs_rebalanced = outputs_augmented\n",
    "\n",
    "n_values = []\n",
    "k_values = []\n",
    "m_values = []\n",
    "P_matrices_flattened = []\n",
    "\n",
    "for sample in inputs_rebalanced:\n",
    "    n_values.append(sample[0])\n",
    "    k_values.append(sample[1])\n",
    "    m_values.append(sample[2])\n",
    "    P_matrices_flattened.append(sample[3].flatten())\n",
    "\n",
    "n_values = np.array(n_values, dtype=np.float32).reshape(-1, 1)\n",
    "k_values = np.array(k_values, dtype=np.int32).reshape(-1, 1)\n",
    "m_values = np.array(m_values, dtype=np.int32).reshape(-1, 1)\n",
    "outputs_array = np.array(outputs_rebalanced, dtype=np.float32)\n",
    "\n",
    "# Pad P matrices\n",
    "max_p_size = max(len(p) for p in P_matrices_flattened)\n",
    "P_matrices_padded = []\n",
    "\n",
    "for p in P_matrices_flattened:\n",
    "    if len(p) < max_p_size:\n",
    "        padded = np.zeros(max_p_size, dtype=np.float32)\n",
    "        padded[:len(p)] = p\n",
    "        P_matrices_padded.append(padded)\n",
    "    else:\n",
    "        P_matrices_padded.append(p)\n",
    "\n",
    "P_matrices = np.array(P_matrices_padded, dtype=np.float32)\n",
    "\n",
    "# Normalize P matrices\n",
    "scaler = StandardScaler()\n",
    "P_matrices_normalized = scaler.fit_transform(P_matrices)\n",
    "\n",
    "outputs_array = np.maximum(outputs_array, 1.0)\n",
    "\n",
    "print(f\"n_values shape: {n_values.shape}\")\n",
    "print(f\"k_values shape: {k_values.shape}, range: [{k_values.min()}, {k_values.max()}]\")\n",
    "print(f\"m_values shape: {m_values.shape}, range: [{m_values.min()}, {m_values.max()}]\")\n",
    "print(f\"P_matrices shape: {P_matrices.shape}\")\n",
    "print(f\"P matrices normalized: mean={P_matrices_normalized.mean():.4f}, std={P_matrices_normalized.std():.4f}\")\n",
    "print(f\"Output (m-height) range: [{outputs_array.min():.2f}, {outputs_array.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Stratified Train-Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 4: Creating Stratified Train-Validation Split\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "stratify_labels = k_values.flatten() * 10 + m_values.flatten()\n",
    "\n",
    "(n_train, n_val,\n",
    " k_train, k_val,\n",
    " m_train, m_val,\n",
    " P_train, P_val,\n",
    " y_train, y_val,\n",
    " strat_train, strat_val) = train_test_split(\n",
    "    n_values, k_values, m_values, P_matrices_normalized, outputs_array,\n",
    "    stratify_labels,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    stratify=stratify_labels\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(y_train)}\")\n",
    "print(f\"Validation samples: {len(y_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define Complexity Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EASY_GROUPS = [(4, 2), (4, 3), (5, 2)]\n",
    "MEDIUM_GROUPS = [(4, 4), (5, 3), (6, 2)]\n",
    "HARD_GROUPS = [(5, 4), (6, 3), (4, 5)]\n",
    "\n",
    "print(f\"Easy groups (Run 2: 0.096-0.108): {EASY_GROUPS}\")\n",
    "print(f\"Medium groups (Run 2: 0.210-0.356): {MEDIUM_GROUPS}\")\n",
    "print(f\"Hard groups (Run 2: 0.958-1.422): {HARD_GROUPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Build Simplified Residual Model\n",
    "\n",
    "Key improvements over Run 4:\n",
    "- NO multi-head attention (removed complexity)\n",
    "- LayerNorm + BatchNorm combination\n",
    "- Only 2 residual blocks (reduced from 3-4)\n",
    "- Progressive width reduction (1024→512→256→128)\n",
    "- Progressive dropout (0.3→0.2→0.1)\n",
    "- Softplus output for log2 constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simplified_residual_model(p_shape, k_vocab_size=7, m_vocab_size=6):\n",
    "    # Inputs\n",
    "    n_input = Input(shape=(1,), name='n')\n",
    "    k_input = Input(shape=(1,), name='k', dtype=tf.int32)\n",
    "    m_input = Input(shape=(1,), name='m', dtype=tf.int32)\n",
    "    P_input = Input(shape=(p_shape,), name='P_flat')\n",
    "\n",
    "    # Embeddings\n",
    "    k_embed = Flatten()(Embedding(k_vocab_size, 32, name='k_embedding')(k_input))\n",
    "    m_embed = Flatten()(Embedding(m_vocab_size, 32, name='m_embedding')(m_input))\n",
    "\n",
    "    # Initial P processing\n",
    "    x = Dense(256, activation='gelu', name='P_initial_1')(P_input)\n",
    "    x = LayerNormalization(name='P_ln1')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(512, activation='gelu', name='P_initial_2')(x)\n",
    "    x = LayerNormalization(name='P_ln2')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    # Combine with embeddings\n",
    "    x = Concatenate(name='combine_embeddings')([n_input, k_embed, m_embed, x])\n",
    "\n",
    "    # Initial dense layer\n",
    "    x = Dense(1024, activation='gelu', name='main_dense1')(x)\n",
    "    x = LayerNormalization(name='main_ln1')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Residual Block 1\n",
    "    residual = x\n",
    "    x = Dense(1024, activation='gelu', name='res1_dense1')(x)\n",
    "    x = LayerNormalization(name='res1_ln1')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1024, activation='gelu', name='res1_dense2')(x)\n",
    "    x = LayerNormalization(name='res1_ln2')(x)\n",
    "    x = Add(name='res1_add')([x, residual])\n",
    "\n",
    "    # Residual Block 2\n",
    "    residual = x\n",
    "    x = Dense(1024, activation='gelu', name='res2_dense1')(x)\n",
    "    x = LayerNormalization(name='res2_ln1')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1024, activation='gelu', name='res2_dense2')(x)\n",
    "    x = LayerNormalization(name='res2_ln2')(x)\n",
    "    x = Add(name='res2_add')([x, residual])\n",
    "\n",
    "    # Progressive width reduction\n",
    "    x = Dense(512, activation='gelu', name='reduction1')(x)\n",
    "    x = LayerNormalization(name='reduction_ln1')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Dense(256, activation='gelu', name='reduction2')(x)\n",
    "    x = LayerNormalization(name='reduction_ln2')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "\n",
    "    x = Dense(128, activation='gelu', name='reduction3')(x)\n",
    "    x = LayerNormalization(name='reduction_ln3')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "\n",
    "    # Output: Predict log2(m-height) then convert to m-height\n",
    "    log2_pred = Dense(1, activation='linear', name='log2_output')(x)\n",
    "    log2_pred_positive = Lambda(lambda z: tf.nn.softplus(z), name='softplus')(log2_pred)\n",
    "    m_height_pred = Lambda(lambda z: tf.pow(2.0, z), name='m_height')(log2_pred_positive)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[n_input, k_input, m_input, P_input],\n",
    "        outputs=m_height_pred,\n",
    "        name='simplified_residual_model_v5'\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "p_shape = P_train.shape[1]\n",
    "model = build_simplified_residual_model(\n",
    "    p_shape,\n",
    "    k_vocab_size=k_values.max()+1,\n",
    "    m_vocab_size=m_values.max()+1\n",
    ")\n",
    "\n",
    "print(f\"Model built successfully!\")\n",
    "print(f\"Total parameters: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log2_mse_loss(y_true, y_pred):\n",
    "    \"\"\"MSE in log2 space\"\"\"\n",
    "    epsilon = 1e-7\n",
    "    y_true = tf.maximum(y_true, epsilon)\n",
    "    y_pred = tf.maximum(y_pred, epsilon)\n",
    "\n",
    "    log2_true = tf.math.log(y_true) / tf.math.log(2.0)\n",
    "    log2_pred = tf.math.log(y_pred) / tf.math.log(2.0)\n",
    "\n",
    "    return tf.reduce_mean(tf.square(log2_true - log2_pred))\n",
    "\n",
    "print(\"Custom log2-MSE loss function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: MILD Group Weighting (CRITICAL FIX!)\n",
    "\n",
    "**CRITICAL:** Run 4 used 5.0x weighting which caused 60-70% degradation in easy cases.  \n",
    "**Run 5:** Use MILD weighting (1.0-2.0x max) to avoid overfitting to hard cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_weights = {\n",
    "    (4, 2): 1.0,   # Easy\n",
    "    (4, 3): 1.0,   # Easy\n",
    "    (4, 4): 1.0,   # Medium\n",
    "    (4, 5): 2.0,   # Hard (was 5.0+ in Run 4 - TOO AGGRESSIVE!)\n",
    "    (5, 2): 1.0,   # Easy\n",
    "    (5, 3): 1.0,   # Medium\n",
    "    (5, 4): 1.5,   # Hard (was 3.0+ in Run 4)\n",
    "    (6, 2): 1.2,   # Medium\n",
    "    (6, 3): 1.5,   # Hard (was 3.0+ in Run 4)\n",
    "}\n",
    "\n",
    "sample_weights_train = np.ones(len(y_train), dtype=np.float32)\n",
    "\n",
    "for i, (k, m) in enumerate(zip(k_train, m_train)):\n",
    "    key = (k[0], m[0])\n",
    "    if key in group_weights:\n",
    "        sample_weights_train[i] = group_weights[key]\n",
    "\n",
    "print(\"Group weights (MILD - not aggressive):\")\n",
    "for (k, m), weight in sorted(group_weights.items()):\n",
    "    print(f\"  k={k}, m={m}: {weight:.1f}x\")\n",
    "print()\n",
    "print(\"CRITICAL: Max weight is 2.0x (was 5.0x+ in Run 4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    clipnorm=1.0  # Gradient clipping\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=log2_mse_loss,\n",
    "    metrics=[log2_mse_loss]\n",
    ")\n",
    "\n",
    "print(\"Model compiled with AdamW optimizer\")\n",
    "print(f\"  Learning rate: 1e-3\")\n",
    "print(f\"  Weight decay: 1e-4\")\n",
    "print(f\"  Gradient clipping: 1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Setup Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    \"\"\"Exponential decay schedule\"\"\"\n",
    "    if epoch < 10:\n",
    "        return lr  # Warmup phase\n",
    "    else:\n",
    "        return lr * 0.95  # Decay by 5% each epoch\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=40,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.7,\n",
    "        patience=15,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_model_run5.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    lr_scheduler\n",
    "]\n",
    "\n",
    "print(\"Callbacks configured:\")\n",
    "print(\"  - EarlyStopping (patience=40)\")\n",
    "print(\"  - ReduceLROnPlateau (patience=15, factor=0.7)\")\n",
    "print(\"  - ModelCheckpoint (best_model_run5.h5)\")\n",
    "print(\"  - Exponential LR decay (0.95 per epoch)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TRAINING MODEL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Batch size: 256\")\n",
    "print(f\"Max epochs: 250\")\n",
    "print(f\"Strategy: Simplified architecture + MILD weighting\")\n",
    "\n",
    "history = model.fit(\n",
    "    [n_train, k_train, m_train, P_train],\n",
    "    y_train,\n",
    "    sample_weight=sample_weights_train,\n",
    "    validation_data=([n_val, k_val, m_val, P_val], y_val),\n",
    "    epochs=250,\n",
    "    batch_size=256,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model_run5.h5')\n",
    "\n",
    "y_pred_train = model.predict([n_train, k_train, m_train, P_train], verbose=0).flatten()\n",
    "y_pred_val = model.predict([n_val, k_val, m_val, P_val], verbose=0).flatten()\n",
    "\n",
    "def compute_log2_mse(y_true, y_pred):\n",
    "    epsilon = 1e-7\n",
    "    y_true = np.maximum(y_true, epsilon)\n",
    "    y_pred = np.maximum(y_pred, epsilon)\n",
    "    log2_true = np.log2(y_true)\n",
    "    log2_pred = np.log2(y_pred)\n",
    "    return np.mean((log2_true - log2_pred) ** 2)\n",
    "\n",
    "train_log2_mse = compute_log2_mse(y_train, y_pred_train)\n",
    "val_log2_mse = compute_log2_mse(y_val, y_pred_val)\n",
    "\n",
    "print(f\"Training log2-MSE: {train_log2_mse:.6f}\")\n",
    "print(f\"Validation log2-MSE: {val_log2_mse:.6f}\")\n",
    "print(f\"Train-Val gap: {abs(train_log2_mse - val_log2_mse):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Per-Group Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_metrics = defaultdict(lambda: {'true': [], 'pred': []})\n",
    "\n",
    "for i in range(len(y_val)):\n",
    "    k = k_val[i, 0]\n",
    "    m = m_val[i, 0]\n",
    "    group_metrics[(k, m)]['true'].append(y_val[i])\n",
    "    group_metrics[(k, m)]['pred'].append(y_pred_val[i])\n",
    "\n",
    "import os\n",
    "os.makedirs('run_5', exist_ok=True)\n",
    "\n",
    "# Reference values\n",
    "run2_reference = {\n",
    "    (4, 2): 0.103043, (4, 3): 0.107653, (4, 4): 0.209655, (4, 5): 1.422106,\n",
    "    (5, 2): 0.096024, (5, 3): 0.227183, (5, 4): 0.957722,\n",
    "    (6, 2): 0.355892, (6, 3): 1.083320\n",
    "}\n",
    "\n",
    "run4_reference = {\n",
    "    (4, 2): 0.167, (4, 3): 0.115, (4, 4): 0.275, (4, 5): 1.379,\n",
    "    (5, 2): 0.152, (5, 3): 0.282, (5, 4): 0.953,\n",
    "    (6, 2): 0.384, (6, 3): 1.072\n",
    "}\n",
    "\n",
    "print(\"\\nValidation Log2-MSE by (k,m) combination:\")\n",
    "print(\"Run 2 | Run 4 | Run 5 | Change from Run 2 | Change from Run 4\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for (k, m), data in sorted(group_metrics.items()):\n",
    "    true_vals = np.array(data['true'])\n",
    "    pred_vals = np.array(data['pred'])\n",
    "    group_log2_mse = compute_log2_mse(true_vals, pred_vals)\n",
    "\n",
    "    run2_val = run2_reference.get((k, m), None)\n",
    "    run4_val = run4_reference.get((k, m), None)\n",
    "\n",
    "    if run2_val:\n",
    "        change_r2 = ((run2_val - group_log2_mse) / run2_val) * 100\n",
    "        change_r2_str = f\"{change_r2:+.1f}%\"\n",
    "    else:\n",
    "        change_r2_str = \"N/A\"\n",
    "\n",
    "    if run4_val:\n",
    "        change_r4 = ((run4_val - group_log2_mse) / run4_val) * 100\n",
    "        change_r4_str = f\"{change_r4:+.1f}%\"\n",
    "    else:\n",
    "        change_r4_str = \"N/A\"\n",
    "\n",
    "    run2_str = f\"{run2_val:.6f}\" if run2_val else \"N/A\"\n",
    "    run4_str = f\"{run4_val:.6f}\" if run4_val else \"N/A\"\n",
    "\n",
    "    print(f\"k={k},m={m}  {run2_str:<10} {run4_str:<10} {group_log2_mse:.6f}  {change_r2_str:<12} {change_r4_str:<12} n={len(true_vals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Generate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training History\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Log2-MSE Loss', fontsize=12)\n",
    "plt.title('Training History - Run 5', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "start_epoch = int(len(history.history['loss']) * 0.2)\n",
    "plt.plot(range(start_epoch, len(history.history['loss'])),\n",
    "         history.history['loss'][start_epoch:],\n",
    "         label='Train Loss', linewidth=2)\n",
    "plt.plot(range(start_epoch, len(history.history['val_loss'])),\n",
    "         history.history['val_loss'][start_epoch:],\n",
    "         label='Val Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Log2-MSE Loss', fontsize=12)\n",
    "plt.title('Training History (Last 80%)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('run_5/training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Predictions Scatter\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_val, y_pred_val, alpha=0.3, s=10)\n",
    "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()],\n",
    "         'r--', linewidth=2, label='Perfect Prediction')\n",
    "plt.xlabel('True m-Height', fontsize=12)\n",
    "plt.ylabel('Predicted m-Height', fontsize=12)\n",
    "plt.title('Predictions vs True Values (Validation Set)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_val, y_pred_val, alpha=0.3, s=10)\n",
    "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()],\n",
    "         'r--', linewidth=2, label='Perfect Prediction')\n",
    "plt.xlabel('True m-Height (log scale)', fontsize=12)\n",
    "plt.ylabel('Predicted m-Height (log scale)', fontsize=12)\n",
    "plt.title('Predictions vs True Values - Log Scale', fontsize=14, fontweight='bold')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('run_5/predictions_scatter.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FINAL RESULTS SUMMARY - RUN 5\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training samples: {len(y_train):,}\")\n",
    "print(f\"Validation samples: {len(y_val):,}\")\n",
    "print(f\"Model parameters: {model.count_params():,}\")\n",
    "print()\n",
    "print(f\"Training log2-MSE:   {train_log2_mse:.6f}\")\n",
    "print(f\"Validation log2-MSE: {val_log2_mse:.6f}\")\n",
    "print(f\"Train-Val gap:       {abs(train_log2_mse - val_log2_mse):.6f}\")\n",
    "print(f\"Target to beat:      0.374000\")\n",
    "print()\n",
    "\n",
    "if val_log2_mse < 0.374:\n",
    "    improvement = ((0.374 - val_log2_mse) / 0.374) * 100\n",
    "    print(f\"✅ SUCCESS! Beat target by {improvement:.1f}%\")\n",
    "    print(f\"   Improvement: {0.374 - val_log2_mse:.6f}\")\n",
    "else:\n",
    "    deficit = ((val_log2_mse - 0.374) / 0.374) * 100\n",
    "    print(f\"⚠️  Did not beat target (worse by {deficit:.1f}%)\")\n",
    "    print(f\"   Need to improve by: {val_log2_mse - 0.374:.6f}\")\n",
    "\n",
    "print()\n",
    "print(\"All predictions ≥ 1.0:\", \"✅ Yes\" if y_pred_val.min() >= 1.0 else \"❌ No\")\n",
    "print(f\"Prediction range: [{y_pred_val.min():.2f}, {y_pred_val.max():.2f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
