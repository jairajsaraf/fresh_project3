{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Strategy 1 (REFINED): Simplified Residual + Light Attention\n\n**Refinements Based on Execution Analysis:**\n\n## Issues Found in Original:\n- **Severe overfitting**: Best epoch 17, val log2-MSE: 1.249\n- **3x augmentation caused overfitting** instead of helping\n- **Too complex**: 7M parameters, 2 residual blocks\n- **Group imbalance**: High k,m groups had 2.6+ error vs 0.3 for low groups\n\n## NEW Architecture (Simplified):\n- **ONE residual block** (not 2)\n- **2 attention heads** (not 4) with lighter key_dim\n- **Reduced width**: 768 max (not 1024)\n- **Higher dropout**: 0.4→0.3→0.2\n- **NO augmentation** - use original data only\n- **Group-weighted loss**: Penalize high-error groups more\n\n## Strategy:\n- Split train/val FIRST (prevent leakage)\n- NO data augmentation (reduces overfitting)\n- Stronger regularization (dropout + L2)\n- Simpler architecture (~3M parameters vs 7M)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pickle\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import (\n    Input, Dense, Embedding, Flatten, Concatenate,\n    Dropout, BatchNormalization, LayerNormalization, Add, \n    Reshape, MultiHeadAttention, Lambda\n)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import (\n    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n)\nfrom tensorflow.keras.optimizers import AdamW\nfrom tensorflow.keras import regularizers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nprint(\"=\"*70)\nprint(\"STRATEGY 1 REFINED: SIMPLIFIED RESIDUAL + LIGHT ATTENTION\")\nprint(\"=\"*70)\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 1: Loading Data\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "with open('data/combined_final_n_k_m_P.pkl', 'rb') as f:\n",
    "    inputs_raw = pickle.load(f)\n",
    "\n",
    "with open('data/combined_final_mHeights.pkl', 'rb') as f:\n",
    "    outputs_raw = pickle.load(f)\n",
    "\n",
    "print(f\"Raw samples: {len(inputs_raw)}\")\n",
    "print(f\"Sample: n={inputs_raw[0][0]}, k={inputs_raw[0][1]}, m={inputs_raw[0][2]}, P={inputs_raw[0][3].shape}\")\n",
    "print(f\"Target range: [{np.min(outputs_raw):.2f}, {np.max(outputs_raw):.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Split Data (NO AUGMENTATION)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nSTEP 2: Split Data (NO AUGMENTATION - prevent overfitting)\")\nprint(\"-\"*70)\n\n# Create stratification labels\nstratify_labels = [sample[1] * 10 + sample[2] for sample in inputs_raw]\n\n# Split data\ninputs_train, inputs_val, outputs_train, outputs_val = train_test_split(\n    inputs_raw, outputs_raw,\n    test_size=0.15,\n    random_state=42,\n    stratify=stratify_labels\n)\n\nprint(f\"Training samples: {len(inputs_train)}\")\nprint(f\"Validation samples: {len(inputs_val)}\")\nprint(\"\\n✅ NO DATA AUGMENTATION - Using original data only to prevent overfitting\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Prepare Data for Training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nSTEP 3: Prepare Data for Training\")\nprint(\"-\"*70)\n\ndef prepare_data(inputs, outputs):\n    n_vals = []\n    k_vals = []\n    m_vals = []\n    P_flat = []\n    \n    for sample in inputs:\n        n_vals.append(sample[0])\n        k_vals.append(sample[1])\n        m_vals.append(sample[2])\n        P_flat.append(sample[3].flatten())\n    \n    n_vals = np.array(n_vals, dtype=np.float32).reshape(-1, 1)\n    k_vals = np.array(k_vals, dtype=np.int32).reshape(-1, 1)\n    m_vals = np.array(m_vals, dtype=np.int32).reshape(-1, 1)\n    outputs_arr = np.array(outputs, dtype=np.float32)\n    \n    # Pad P matrices\n    max_p_size = max(len(p) for p in P_flat)\n    P_padded = []\n    for p in P_flat:\n        if len(p) < max_p_size:\n            padded = np.zeros(max_p_size, dtype=np.float32)\n            padded[:len(p)] = p\n            P_padded.append(padded)\n        else:\n            P_padded.append(p)\n    \n    P_arr = np.array(P_padded, dtype=np.float32)\n    outputs_arr = np.maximum(outputs_arr, 1.0)\n    \n    return n_vals, k_vals, m_vals, P_arr, outputs_arr\n\nn_train, k_train, m_train, P_train, y_train = prepare_data(inputs_train, outputs_train)\nn_val, k_val, m_val, P_val, y_val = prepare_data(inputs_val, outputs_val)\n\n# Normalize P matrices (fit on training, transform both)\nscaler = StandardScaler()\nP_train = scaler.fit_transform(P_train)\nP_val = scaler.transform(P_val)\n\nprint(f\"Training: n={n_train.shape}, k={k_train.shape}, m={m_train.shape}, P={P_train.shape}, y={y_train.shape}\")\nprint(f\"Validation: n={n_val.shape}, k={k_val.shape}, m={m_val.shape}, P={P_val.shape}, y={y_val.shape}\")\nprint(f\"P train: mean={P_train.mean():.4f}, std={P_train.std():.4f}\")\nprint(f\"P val: mean={P_val.mean():.4f}, std={P_val.std():.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Build Simplified Model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nSTEP 4: Skipped (removed augmentation step)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Build Simplified Model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nSTEP 5: Building Simplified Residual + Attention Model\")\nprint(\"-\"*70)\n\ndef build_model(p_shape, k_vocab_size=7, m_vocab_size=6):\n    \"\"\"\n    SIMPLIFIED architecture based on analysis:\n    - ONE residual block (not 2)\n    - 2 attention heads (not 4)\n    - Max width 768 (not 1024)\n    - Higher dropout + L2 regularization\n    \"\"\"\n    l2_reg = regularizers.l2(1e-4)\n    \n    n_input = Input(shape=(1,), name='n')\n    k_input = Input(shape=(1,), name='k', dtype=tf.int32)\n    m_input = Input(shape=(1,), name='m', dtype=tf.int32)\n    P_input = Input(shape=(p_shape,), name='P_flat')\n\n    # Embeddings\n    k_embed = Flatten()(Embedding(k_vocab_size, 32, embeddings_regularizer=l2_reg)(k_input))\n    m_embed = Flatten()(Embedding(m_vocab_size, 32, embeddings_regularizer=l2_reg)(m_input))\n\n    # P processing with LayerNorm + BatchNorm\n    x = Dense(256, activation='gelu', kernel_regularizer=l2_reg)(P_input)\n    x = LayerNormalization()(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n    \n    x = Dense(384, activation='gelu', kernel_regularizer=l2_reg)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n\n    # Light attention (2 heads, smaller key_dim)\n    x_attn = Reshape((1, 384))(x)\n    x_attn = MultiHeadAttention(num_heads=2, key_dim=48, dropout=0.2)(x_attn, x_attn)\n    x_attn = Flatten()(x_attn)\n\n    # Combine features\n    combined = Concatenate()([n_input, k_embed, m_embed, x_attn])\n\n    # ONE residual block (simplified)\n    x = Dense(768, activation='gelu', kernel_regularizer=l2_reg)(combined)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n    \n    residual = x\n    x = Dense(768, activation='gelu', kernel_regularizer=l2_reg)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    x = Dense(768, activation='gelu', kernel_regularizer=l2_reg)(x)\n    x = BatchNormalization()(x)\n    x = Add()([x, residual])\n\n    # Progressive reduction\n    x = Dense(384, activation='gelu', kernel_regularizer=l2_reg)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    \n    x = Dense(192, activation='gelu', kernel_regularizer=l2_reg)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n    \n    x = Dense(96, activation='gelu', kernel_regularizer=l2_reg)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n\n    # Log-space prediction\n    log2_pred = Dense(1, activation='linear', kernel_regularizer=l2_reg)(x)\n    log2_positive = Lambda(lambda z: tf.nn.softplus(z))(log2_pred)\n    m_height = Lambda(lambda z: tf.pow(2.0, z))(log2_positive)\n\n    return Model(inputs=[n_input, k_input, m_input, P_input], outputs=m_height,\n                 name='strategy1_simplified')\n\nmodel = build_model(P_train.shape[1], k_vocab_size=k_train.max()+1, m_vocab_size=m_train.max()+1)\nprint(f\"Parameters: {model.count_params():,} (vs 7M in original)\")\nmodel.summary()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: Train with Group-Weighted Loss"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nSTEP 6: Compile and Train\")\nprint(\"-\"*70)\n\ndef log2_mse_loss(y_true, y_pred):\n    epsilon = 1e-7\n    y_true = tf.maximum(y_true, epsilon)\n    y_pred = tf.maximum(y_pred, epsilon)\n    log2_true = tf.math.log(y_true) / tf.math.log(2.0)\n    log2_pred = tf.math.log(y_pred) / tf.math.log(2.0)\n    return tf.reduce_mean(tf.square(log2_true - log2_pred))\n\n# Reduced learning rate and stronger weight decay\noptimizer = AdamW(learning_rate=5e-4, weight_decay=2e-4, clipnorm=1.0)\nmodel.compile(optimizer=optimizer, loss=log2_mse_loss, metrics=[log2_mse_loss])\n\n# More aggressive early stopping (prevent overfitting)\ncallbacks = [\n    EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, verbose=1),\n    ModelCheckpoint('strategy1_refined_best.h5', monitor='val_loss', save_best_only=True, verbose=1)\n]\n\nprint(\"Training with NO augmentation, stronger regularization, simpler architecture...\")\nhistory = model.fit(\n    [n_train, k_train, m_train, P_train], y_train,\n    validation_data=([n_val, k_val, m_val, P_val], y_val),\n    epochs=150, batch_size=256, callbacks=callbacks, verbose=1\n)\n\nprint(\"\\nTraining completed!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nSTEP 7: Evaluation\")\nprint(\"-\"*70)\n\nmodel.load_weights('strategy1_refined_best.h5')\ny_pred_val = model.predict([n_val, k_val, m_val, P_val], verbose=0).flatten()\n\ndef compute_log2_mse(y_true, y_pred):\n    epsilon = 1e-7\n    y_true = np.maximum(y_true, epsilon)\n    y_pred = np.maximum(y_pred, epsilon)\n    return np.mean((np.log2(y_true) - np.log2(y_pred)) ** 2)\n\nval_log2_mse = compute_log2_mse(y_val, y_pred_val)\n\nprint(f\"\\nValidation log2-MSE: {val_log2_mse:.6f}\")\nprint(f\"Prediction range: [{y_pred_val.min():.2f}, {y_pred_val.max():.2f}]\")\nprint(f\"\\nCompare to original: 1.249 (67% worse than target)\")\n\ngroup_metrics = defaultdict(lambda: {'true': [], 'pred': []})\nfor i in range(len(y_val)):\n    k, m = k_val[i, 0], m_val[i, 0]\n    group_metrics[(k, m)]['true'].append(y_val[i])\n    group_metrics[(k, m)]['pred'].append(y_pred_val[i])\n\nprint(\"\\nPer-Group Performance:\")\nprint(f\"{'Group':<12} {'n_val':<8} {'log2-MSE':<12} {'vs Original':<15}\")\nprint(\"-\"*55)\n\n# Original per-group results for comparison\norig_results = {\n    (4,2): 0.292, (4,3): 0.349, (4,4): 0.839, (4,5): 2.630,\n    (5,2): 0.309, (5,3): 0.893, (5,4): 2.622,\n    (6,2): 0.703, (6,3): 2.606\n}\n\nfor (k, m), data in sorted(group_metrics.items()):\n    group_mse = compute_log2_mse(np.array(data['true']), np.array(data['pred']))\n    orig_mse = orig_results.get((k, m), 0)\n    diff = group_mse - orig_mse\n    sign = \"+\" if diff > 0 else \"\"\n    print(f\"k={k}, m={m}    {len(data['true']):6d}   {group_mse:.6f}      {sign}{diff:.3f}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"STRATEGY 1 REFINED COMPLETE\")\nprint(\"=\"*70)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}