{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy 1: Advanced Residual + Selective Attention\n",
    "\n",
    "**Based on Professor's Top Submissions (Avg Score: 77.8)**\n",
    "\n",
    "## Architecture Features:\n",
    "- Residual connections with skip paths\n",
    "- Selective multi-head attention (1 attention block)\n",
    "- LayerNorm + BatchNorm combinations\n",
    "- Log-space prediction (log2)\n",
    "- Progressive dropout (0.3 → 0.2 → 0.1)\n",
    "- AdamW optimizer with weight decay\n",
    "\n",
    "## Data Strategy:\n",
    "- Heavy augmentation: 3x multiplier (108K → 324K samples)\n",
    "- Techniques: Gaussian noise, perturbations, SMOTE-like interpolation\n",
    "- Balanced across all (k,m) groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Embedding, Flatten, Concatenate,\n",
    "    Dropout, BatchNormalization, LayerNormalization, Add, \n",
    "    Reshape, MultiHeadAttention, Lambda\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    ")\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STRATEGY 1: ADVANCED RESIDUAL + SELECTIVE ATTENTION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 1: Loading Data\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "with open('data/combined_final_n_k_m_P.pkl', 'rb') as f:\n",
    "    inputs_raw = pickle.load(f)\n",
    "\n",
    "with open('data/combined_final_mHeights.pkl', 'rb') as f:\n",
    "    outputs_raw = pickle.load(f)\n",
    "\n",
    "print(f\"Raw samples: {len(inputs_raw)}\")\n",
    "print(f\"Sample structure: n={inputs_raw[0][0]}, k={inputs_raw[0][1]}, m={inputs_raw[0][2]}, P shape={inputs_raw[0][3].shape}\")\n",
    "print(f\"Target range: [{np.min(outputs_raw):.2f}, {np.max(outputs_raw):.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Heavy Data Augmentation (3x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 2: Heavy Data Augmentation (3x multiplier)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "def augment_sample_gaussian(n, k, m, P, target, noise_level=0.03):\n",
    "    \"\"\"Add Gaussian noise to P matrix\"\"\"\n",
    "    P_aug = P.copy().astype(np.float32)\n",
    "    noise = np.random.normal(0, noise_level, P_aug.shape)\n",
    "    P_aug = P_aug + noise * np.std(P_aug)\n",
    "    return [n, k, m, P_aug], target\n",
    "\n",
    "def augment_sample_perturbation(n, k, m, P, target, strength=0.02):\n",
    "    \"\"\"Apply multiplicative perturbations\"\"\"\n",
    "    P_aug = P.copy().astype(np.float32)\n",
    "    perturbation = np.random.uniform(-strength, strength, P_aug.shape)\n",
    "    P_aug = P_aug * (1 + perturbation)\n",
    "    return [n, k, m, P_aug], target\n",
    "\n",
    "def augment_sample_interpolation(sample1, sample2, target1, target2):\n",
    "    \"\"\"SMOTE-like interpolation between samples\"\"\"\n",
    "    alpha = np.random.uniform(0.3, 0.7)\n",
    "    n, k, m = sample1[0], sample1[1], sample1[2]\n",
    "    P1, P2 = sample1[3], sample2[3]\n",
    "    \n",
    "    # Only interpolate if same (k,m)\n",
    "    if sample1[1] == sample2[1] and sample1[2] == sample2[2]:\n",
    "        P_new = alpha * P1.astype(np.float32) + (1 - alpha) * P2.astype(np.float32)\n",
    "        target_new = alpha * target1 + (1 - alpha) * target2\n",
    "        return [n, k, m, P_new], target_new\n",
    "    return None, None\n",
    "\n",
    "# Group by (k,m)\n",
    "groups = defaultdict(list)\n",
    "for i, sample in enumerate(inputs_raw):\n",
    "    k, m = sample[1], sample[2]\n",
    "    groups[(k, m)].append(i)\n",
    "\n",
    "inputs_augmented = []\n",
    "outputs_augmented = []\n",
    "\n",
    "# For each sample, create 2 augmented copies (1 original + 2 aug = 3x total)\n",
    "for i, (sample, target) in enumerate(zip(inputs_raw, outputs_raw)):\n",
    "    n, k, m, P = sample\n",
    "    \n",
    "    # Keep original\n",
    "    inputs_augmented.append(sample)\n",
    "    outputs_augmented.append(target)\n",
    "    \n",
    "    # Augmentation 1: Gaussian noise\n",
    "    aug1, tgt1 = augment_sample_gaussian(n, k, m, P, target)\n",
    "    inputs_augmented.append(aug1)\n",
    "    outputs_augmented.append(tgt1)\n",
    "    \n",
    "    # Augmentation 2: Perturbation or interpolation\n",
    "    if np.random.rand() < 0.5:\n",
    "        aug2, tgt2 = augment_sample_perturbation(n, k, m, P, target)\n",
    "        inputs_augmented.append(aug2)\n",
    "        outputs_augmented.append(tgt2)\n",
    "    else:\n",
    "        # Try interpolation with random sample from same group\n",
    "        group_indices = groups[(k, m)]\n",
    "        if len(group_indices) > 1:\n",
    "            j = np.random.choice([idx for idx in group_indices if idx != i])\n",
    "            aug2, tgt2 = augment_sample_interpolation(\n",
    "                sample, inputs_raw[j], target, outputs_raw[j]\n",
    "            )\n",
    "            if aug2 is not None:\n",
    "                inputs_augmented.append(aug2)\n",
    "                outputs_augmented.append(tgt2)\n",
    "            else:\n",
    "                # Fallback to perturbation\n",
    "                aug2, tgt2 = augment_sample_perturbation(n, k, m, P, target)\n",
    "                inputs_augmented.append(aug2)\n",
    "                outputs_augmented.append(tgt2)\n",
    "        else:\n",
    "            aug2, tgt2 = augment_sample_perturbation(n, k, m, P, target)\n",
    "            inputs_augmented.append(aug2)\n",
    "            outputs_augmented.append(tgt2)\n",
    "\n",
    "print(f\"Original samples: {len(inputs_raw)}\")\n",
    "print(f\"Augmented samples: {len(inputs_augmented)}\")\n",
    "print(f\"Augmentation ratio: {len(inputs_augmented) / len(inputs_raw):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 3: Preparing Data for Training\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "n_values = []\n",
    "k_values = []\n",
    "m_values = []\n",
    "P_matrices_flattened = []\n",
    "\n",
    "for sample in inputs_augmented:\n",
    "    n_values.append(sample[0])\n",
    "    k_values.append(sample[1])\n",
    "    m_values.append(sample[2])\n",
    "    P_matrices_flattened.append(sample[3].flatten())\n",
    "\n",
    "n_values = np.array(n_values, dtype=np.float32).reshape(-1, 1)\n",
    "k_values = np.array(k_values, dtype=np.int32).reshape(-1, 1)\n",
    "m_values = np.array(m_values, dtype=np.int32).reshape(-1, 1)\n",
    "outputs_array = np.array(outputs_augmented, dtype=np.float32)\n",
    "\n",
    "# Pad P matrices\n",
    "max_p_size = max(len(p) for p in P_matrices_flattened)\n",
    "P_matrices_padded = []\n",
    "for p in P_matrices_flattened:\n",
    "    if len(p) < max_p_size:\n",
    "        padded = np.zeros(max_p_size, dtype=np.float32)\n",
    "        padded[:len(p)] = p\n",
    "        P_matrices_padded.append(padded)\n",
    "    else:\n",
    "        P_matrices_padded.append(p)\n",
    "\n",
    "P_matrices = np.array(P_matrices_padded, dtype=np.float32)\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "P_matrices_normalized = scaler.fit_transform(P_matrices)\n",
    "outputs_array = np.maximum(outputs_array, 1.0)\n",
    "\n",
    "print(f\"Data shapes: n={n_values.shape}, k={k_values.shape}, m={m_values.shape}, P={P_matrices.shape}\")\n",
    "print(f\"P normalized: mean={P_matrices_normalized.mean():.4f}, std={P_matrices_normalized.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train-Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 4: Stratified Train-Validation Split\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "stratify_labels = k_values.flatten() * 10 + m_values.flatten()\n",
    "\n",
    "(n_train, n_val, k_train, k_val, m_train, m_val, P_train, P_val, \n",
    " y_train, y_val, _, _) = train_test_split(\n",
    "    n_values, k_values, m_values, P_matrices_normalized, outputs_array, stratify_labels,\n",
    "    test_size=0.15, random_state=42, stratify=stratify_labels\n",
    ")\n",
    "\n",
    "print(f\"Training: {len(y_train):,} samples\")\n",
    "print(f\"Validation: {len(y_val):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build Advanced Residual + Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 5: Building Advanced Residual + Attention Model\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "def build_advanced_residual_attention_model(p_shape, k_vocab_size=7, m_vocab_size=6):\n",
    "    \"\"\"\n",
    "    Strategy 1: Advanced Residual + Selective Attention (Score: 77.8)\n",
    "    - Residual connections with skip paths\n",
    "    - Selective multi-head attention (1 attention block)\n",
    "    - LayerNorm + BatchNorm combinations\n",
    "    - Progressive dropout (0.3 → 0.2 → 0.1)\n",
    "    \"\"\"\n",
    "    # Inputs\n",
    "    n_input = Input(shape=(1,), name='n')\n",
    "    k_input = Input(shape=(1,), name='k', dtype=tf.int32)\n",
    "    m_input = Input(shape=(1,), name='m', dtype=tf.int32)\n",
    "    P_input = Input(shape=(p_shape,), name='P_flat')\n",
    "\n",
    "    # Embeddings\n",
    "    k_embed = Flatten()(Embedding(k_vocab_size, 32)(k_input))\n",
    "    m_embed = Flatten()(Embedding(m_vocab_size, 32)(m_input))\n",
    "\n",
    "    # P processing with LayerNorm + BatchNorm\n",
    "    x = Dense(256, activation='gelu')(P_input)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(512, activation='gelu')(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    # Selective Multi-Head Attention (1 block)\n",
    "    x_attn = Reshape((1, 512))(x)\n",
    "    x_attn = MultiHeadAttention(num_heads=4, key_dim=64, dropout=0.1)(x_attn, x_attn)\n",
    "    x_attn = Flatten()(x_attn)\n",
    "\n",
    "    # Combine features\n",
    "    combined = Concatenate()([n_input, k_embed, m_embed, x_attn])\n",
    "\n",
    "    # Residual Block 1\n",
    "    x = Dense(1024, activation='gelu')(combined)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    residual = x\n",
    "    x = Dense(1024, activation='gelu')(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1024, activation='gelu')(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Add()([x, residual])  # Skip connection\n",
    "\n",
    "    # Residual Block 2\n",
    "    residual = x\n",
    "    x = Dense(1024, activation='gelu')(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1024, activation='gelu')(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Add()([x, residual])  # Skip connection\n",
    "\n",
    "    # Progressive width reduction\n",
    "    x = Dense(512, activation='gelu')(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(256, activation='gelu')(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    x = Dense(128, activation='gelu')(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "\n",
    "    # Log-space prediction with Softplus\n",
    "    log2_pred = Dense(1, activation='linear')(x)\n",
    "    log2_positive = Lambda(lambda z: tf.nn.softplus(z))(log2_pred)\n",
    "    m_height = Lambda(lambda z: tf.pow(2.0, z))(log2_positive)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[n_input, k_input, m_input, P_input],\n",
    "        outputs=m_height,\n",
    "        name='strategy1_advanced_residual_attention'\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_advanced_residual_attention_model(\n",
    "    P_train.shape[1],\n",
    "    k_vocab_size=k_values.max()+1,\n",
    "    m_vocab_size=m_values.max()+1\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {model.count_params():,}\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Compile & Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 6: Compile and Train\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "def log2_mse_loss(y_true, y_pred):\n",
    "    epsilon = 1e-7\n",
    "    y_true = tf.maximum(y_true, epsilon)\n",
    "    y_pred = tf.maximum(y_pred, epsilon)\n",
    "    log2_true = tf.math.log(y_true) / tf.math.log(2.0)\n",
    "    log2_pred = tf.math.log(y_pred) / tf.math.log(2.0)\n",
    "    return tf.reduce_mean(tf.square(log2_true - log2_pred))\n",
    "\n",
    "optimizer = AdamW(learning_rate=1e-3, weight_decay=1e-4, clipnorm=1.0)\n",
    "model.compile(optimizer=optimizer, loss=log2_mse_loss, metrics=[log2_mse_loss])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=15, min_lr=1e-6, verbose=1),\n",
    "    ModelCheckpoint('strategy1_best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    [n_train, k_train, m_train, P_train], y_train,\n",
    "    validation_data=([n_val, k_val, m_val, P_val], y_val),\n",
    "    epochs=200, batch_size=256, callbacks=callbacks, verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 7: Evaluation\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "model.load_weights('strategy1_best_model.h5')\n",
    "y_pred_val = model.predict([n_val, k_val, m_val, P_val], verbose=0).flatten()\n",
    "\n",
    "def compute_log2_mse(y_true, y_pred):\n",
    "    epsilon = 1e-7\n",
    "    y_true = np.maximum(y_true, epsilon)\n",
    "    y_pred = np.maximum(y_pred, epsilon)\n",
    "    log2_true = np.log2(y_true)\n",
    "    log2_pred = np.log2(y_pred)\n",
    "    return np.mean((log2_true - log2_pred) ** 2)\n",
    "\n",
    "val_log2_mse = compute_log2_mse(y_val, y_pred_val)\n",
    "\n",
    "print(f\"\\nValidation log2-MSE: {val_log2_mse:.6f}\")\n",
    "print(f\"Prediction range: [{y_pred_val.min():.2f}, {y_pred_val.max():.2f}]\")\n",
    "\n",
    "# Per-group analysis\n",
    "group_metrics = defaultdict(lambda: {'true': [], 'pred': []})\n",
    "for i in range(len(y_val)):\n",
    "    k, m = k_val[i, 0], m_val[i, 0]\n",
    "    group_metrics[(k, m)]['true'].append(y_val[i])\n",
    "    group_metrics[(k, m)]['pred'].append(y_pred_val[i])\n",
    "\n",
    "print(\"\\nPer-Group Performance:\")\n",
    "print(f\"{'Group':<12} {'n_val':<8} {'log2-MSE':<12}\")\n",
    "print(\"-\"*40)\n",
    "for (k, m), data in sorted(group_metrics.items()):\n",
    "    true_vals = np.array(data['true'])\n",
    "    pred_vals = np.array(data['pred'])\n",
    "    group_mse = compute_log2_mse(true_vals, pred_vals)\n",
    "    print(f\"k={k}, m={m}    {len(true_vals):6d}   {group_mse:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STRATEGY 1 COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
